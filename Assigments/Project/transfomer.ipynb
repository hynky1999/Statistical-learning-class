{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torchmetrics datasets tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset wmt14 (/home/kydliceh/.cache/huggingface/datasets/wmt14/de-en/1.0.0/2de185b074515e97618524d69f5e27ee7545dcbed4aa9bc1a4235710ffca33f4)\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.03169131278991699,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 3,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "598abd1fd87e429e92630c7e2a7a4fdf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "wmt14 = load_dataset('wmt14', 'de-en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_subset_length = 1000\n",
    "test_subset_length = 100\n",
    "vocab_size=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = wmt14['train'].select(range(train_subset_length))\n",
    "test_dataset = wmt14['train'].select(range(test_subset_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers.models import BPE\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.processors import TemplateProcessing\n",
    "\n",
    "def create_tokenizer(iterable, add_special_tokens=False):\n",
    "    trainer = BpeTrainer(vocab_size=vocab_size, show_progress=True, special_tokens=[\"[PAD]\",\"[UNK]\"])\n",
    "    tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
    "    tokenizer.train_from_iterator(iterable, trainer=trainer)\n",
    "    if add_special_tokens:\n",
    "        tokenizer.add_special_tokens([\"[START]\", \"[END]\"])\n",
    "        START_ID, END_ID = tokenizer.token_to_id(\"[START]\"), tokenizer.token_to_id(\"[END]\")\n",
    "        tokenizer.post_processor = TemplateProcessing(single=\"[START] $A [END]\", special_tokens=[(\"[START]\", START_ID), (\"[END]\", END_ID)])\n",
    "\n",
    "\n",
    "    tokenizer.enable_padding(pad_token=\"[PAD]\", pad_id=tokenizer.token_to_id(\"[PAD]\"))\n",
    "    return tokenizer\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "de_it = map(lambda x: x['de'] , train_dataset['translation'])\n",
    "en_it = map(lambda x: x['en'] , train_dataset['translation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "de_token = create_tokenizer(de_it, add_special_tokens=True)\n",
    "en_token = create_tokenizer(en_it)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "def extract_embedding(embeds, lang):\n",
    "    return {f\"{lang}_ids\": [e.ids for e in embeds], f\"{lang}_att\": [e.attention_mask for e in embeds]}\n",
    "\n",
    "\n",
    "def tokenize(trans):\n",
    "    translation = trans[\"translation\"]\n",
    "    de_sent = [t[\"de\"] for t in translation]\n",
    "    en_sent = [t[\"en\"] for t in translation]\n",
    "    en = en_token.encode_batch(en_sent)\n",
    "    de = de_token.encode_batch(de_sent)\n",
    "    dct = {**extract_embedding(en, \"en\"), **extract_embedding(de, \"de\"), \"de_sent\": de_sent, \"en_sent\": en_sent}\n",
    "    return dct\n",
    "\n",
    "def collate_fc(batch):\n",
    "    en_ids = torch.stack([b[\"en_ids\"] for b in batch])\n",
    "    en_att = torch.stack([b[\"en_att\"] for b in batch]).unsqueeze(1).unsqueeze(1)\n",
    "    de_ids = torch.stack([b[\"de_ids\"] for b in batch])\n",
    "    de_att = torch.stack([b[\"de_att\"] for b in batch]).unsqueeze(1).unsqueeze(1)\n",
    "    de_sent = [b[\"de_sent\"] for b in batch]\n",
    "    en_sent = [b[\"en_sent\"] for b in batch]\n",
    "\n",
    "    return {\"en_ids\": en_ids, \"de_ids\": de_ids, \"en_att\": en_att, \"de_att\": de_att, \"de_sent\": de_sent, \"en_sent\": en_sent}\n",
    "\n",
    "def create_dataloader(dataset, batch_size=32, shuffle=False):\n",
    "    tokenized = dataset.map(tokenize, batch_size=1, batched=True)\n",
    "    tokenized = tokenized.remove_columns(\"translation\")\n",
    "    tokenized.set_format(\"torch\")\n",
    "    return DataLoader(tokenized, batch_size=batch_size, shuffle=False,collate_fn=collate_fc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.03830599784851074,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 1000,
       "unit": "ba",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e121b7710f3f432e94a9415cccb2e8e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.035387516021728516,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 1,
       "unit": "ba",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76f1bebec6484c4b81a9482be701bb96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataloader_train = create_dataloader(train_dataset, batch_size=1, shuffle=False)\n",
    "dataloader_test = create_dataloader(test_dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train_test import train\n",
    "import torch.nn as nn\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress/train 0: 0/0 Loss: 0.048531694412231444, Time: 0.08668875694274902\n",
      "Progress/train 1: 0/0 Loss: 0.045637073516845705, Time: 0.07168769836425781\n",
      "Progress/train 2: 0/0 Loss: 0.04788431167602539, Time: 0.07259058952331543\n",
      "Progress/train 3: 0/0 Loss: 0.04609570503234863, Time: 0.07016372680664062\n",
      "Progress/train 4: 0/0 Loss: 0.04670266151428223, Time: 0.0716712474822998\n",
      "Progress/train 5: 0/0 Loss: 0.04629613876342773, Time: 0.07016181945800781\n",
      "Progress/train 6: 0/0 Loss: 0.04715780735015869, Time: 0.08602237701416016\n",
      "Progress/train 7: 0/0 Loss: 0.04675246715545654, Time: 0.1008615493774414\n",
      "Progress/train 8: 0/0 Loss: 0.04590414524078369, Time: 0.09317326545715332\n",
      "Progress/train 9: 0/0 Loss: 0.04648094654083252, Time: 0.11342144012451172\n",
      "Progress/train 10: 0/0 Loss: 0.04826240062713623, Time: 0.10730195045471191\n",
      "Progress/train 11: 0/0 Loss: 0.047436800003051754, Time: 0.12381649017333984\n",
      "Progress/train 12: 0/0 Loss: 0.04618993282318115, Time: 0.11117148399353027\n",
      "Progress/train 13: 0/0 Loss: 0.046759729385375974, Time: 0.12443780899047852\n",
      "Progress/train 14: 0/0 Loss: 0.04625020503997803, Time: 0.10521721839904785\n",
      "Progress/train 15: 0/0 Loss: 0.04664808750152588, Time: 0.07733345031738281\n",
      "Progress/train 16: 0/0 Loss: 0.04642003059387207, Time: 0.07474660873413086\n",
      "Progress/train 17: 0/0 Loss: 0.046651296615600586, Time: 0.08699679374694824\n",
      "Progress/train 18: 0/0 Loss: 0.046530170440673826, Time: 0.08416223526000977\n",
      "Progress/train 19: 0/0 Loss: 0.04679614543914795, Time: 0.08803272247314453\n",
      "Progress/train 20: 0/0 Loss: 0.04484579086303711, Time: 0.08121490478515625\n",
      "Progress/train 21: 0/0 Loss: 0.047284836769104006, Time: 0.07047319412231445\n",
      "Progress/train 22: 0/0 Loss: 0.04476035594940186, Time: 0.0902550220489502\n",
      "Progress/train 23: 0/0 Loss: 0.045522456169128415, Time: 0.07810449600219727\n",
      "Progress/train 24: 0/0 Loss: 0.046573848724365235, Time: 0.07553768157958984\n",
      "Progress/train 25: 0/0 Loss: 0.04464837074279785, Time: 0.09032797813415527\n",
      "Progress/train 26: 0/0 Loss: 0.04724659442901611, Time: 0.1301100254058838\n",
      "Progress/train 27: 0/0 Loss: 0.046773309707641604, Time: 0.1560208797454834\n",
      "Progress/train 28: 0/0 Loss: 0.0484392786026001, Time: 0.1006932258605957\n",
      "Progress/train 29: 0/0 Loss: 0.045489959716796875, Time: 0.09951210021972656\n",
      "Progress/train 30: 0/0 Loss: 0.04944783687591553, Time: 0.09918355941772461\n",
      "Progress/train 31: 0/0 Loss: 0.04580524921417237, Time: 0.07554340362548828\n",
      "Progress/train 32: 0/0 Loss: 0.0467801284790039, Time: 0.10332584381103516\n",
      "Progress/train 33: 0/0 Loss: 0.046647744178771974, Time: 0.0884857177734375\n",
      "Progress/train 34: 0/0 Loss: 0.044028067588806154, Time: 0.07037091255187988\n",
      "Progress/train 35: 0/0 Loss: 0.047062435150146485, Time: 0.07562375068664551\n",
      "Progress/train 36: 0/0 Loss: 0.04469686508178711, Time: 0.0703577995300293\n",
      "Progress/train 37: 0/0 Loss: 0.04739130973815918, Time: 0.07548880577087402\n",
      "Progress/train 38: 0/0 Loss: 0.045868358612060546, Time: 0.07761263847351074\n",
      "Progress/train 39: 0/0 Loss: 0.046181740760803225, Time: 0.07327985763549805\n",
      "Progress/train 40: 0/0 Loss: 0.04531559944152832, Time: 0.08412837982177734\n",
      "Progress/train 41: 0/0 Loss: 0.046376953125, Time: 0.13474535942077637\n",
      "Progress/train 42: 0/0 Loss: 0.04475432872772217, Time: 0.0911102294921875\n",
      "Progress/train 43: 0/0 Loss: 0.04482181072235107, Time: 0.07110118865966797\n",
      "Progress/train 44: 0/0 Loss: 0.04744539260864258, Time: 0.07929635047912598\n",
      "Progress/train 45: 0/0 Loss: 0.045397257804870604, Time: 0.07683300971984863\n",
      "Progress/train 46: 0/0 Loss: 0.04492706775665283, Time: 0.0738229751586914\n",
      "Progress/train 47: 0/0 Loss: 0.04607736110687256, Time: 0.0905604362487793\n",
      "Progress/train 48: 0/0 Loss: 0.046713099479675294, Time: 0.09640955924987793\n",
      "Progress/train 49: 0/0 Loss: 0.04542559623718262, Time: 0.08930468559265137\n",
      "Progress/train 50: 0/0 Loss: 0.04666969776153564, Time: 0.09533357620239258\n",
      "Progress/train 51: 0/0 Loss: 0.04492212295532227, Time: 0.06536984443664551\n",
      "Progress/train 52: 0/0 Loss: 0.047156844139099124, Time: 0.07120990753173828\n",
      "Progress/train 53: 0/0 Loss: 0.044802765846252444, Time: 0.0715491771697998\n",
      "Progress/train 54: 0/0 Loss: 0.046333813667297365, Time: 0.07600593566894531\n",
      "Progress/train 55: 0/0 Loss: 0.04641768455505371, Time: 0.08750224113464355\n",
      "Progress/train 56: 0/0 Loss: 0.046896753311157224, Time: 0.13357782363891602\n",
      "Progress/train 57: 0/0 Loss: 0.04581181049346924, Time: 0.10113525390625\n",
      "Progress/train 58: 0/0 Loss: 0.045425100326538084, Time: 0.08276534080505371\n",
      "Progress/train 59: 0/0 Loss: 0.04476011276245117, Time: 0.08407878875732422\n",
      "Progress/train 60: 0/0 Loss: 0.046904706954956056, Time: 0.0691366195678711\n",
      "Progress/train 61: 0/0 Loss: 0.046598925590515136, Time: 0.07333064079284668\n",
      "Progress/train 62: 0/0 Loss: 0.04594353675842285, Time: 0.09436726570129395\n",
      "Progress/train 63: 0/0 Loss: 0.046279296875, Time: 0.07924151420593262\n",
      "Progress/train 64: 0/0 Loss: 0.045855226516723635, Time: 0.07356929779052734\n",
      "Progress/train 65: 0/0 Loss: 0.04458790779113769, Time: 0.10039448738098145\n",
      "Progress/train 66: 0/0 Loss: 0.04540440559387207, Time: 0.06616640090942383\n",
      "Progress/train 67: 0/0 Loss: 0.045232839584350586, Time: 0.06443428993225098\n",
      "Progress/train 68: 0/0 Loss: 0.04568371772766113, Time: 0.08080792427062988\n",
      "Progress/train 69: 0/0 Loss: 0.046995353698730466, Time: 0.08027148246765137\n",
      "Progress/train 70: 0/0 Loss: 0.04608542442321777, Time: 0.09408211708068848\n",
      "Progress/train 71: 0/0 Loss: 0.04439027309417724, Time: 0.104034423828125\n",
      "Progress/train 72: 0/0 Loss: 0.04524221420288086, Time: 0.11045718193054199\n",
      "Progress/train 73: 0/0 Loss: 0.04498067855834961, Time: 0.10245203971862793\n",
      "Progress/train 74: 0/0 Loss: 0.04370701313018799, Time: 0.08457088470458984\n",
      "Progress/train 75: 0/0 Loss: 0.043660874366760256, Time: 0.07895922660827637\n",
      "Progress/train 76: 0/0 Loss: 0.046893038749694825, Time: 0.102783203125\n",
      "Progress/train 77: 0/0 Loss: 0.044808688163757326, Time: 0.07271909713745117\n",
      "Progress/train 78: 0/0 Loss: 0.04557722568511963, Time: 0.06858205795288086\n",
      "Progress/train 79: 0/0 Loss: 0.04429347515106201, Time: 0.07949995994567871\n",
      "Progress/train 80: 0/0 Loss: 0.043386225700378415, Time: 0.08825016021728516\n",
      "Progress/train 81: 0/0 Loss: 0.04463286399841308, Time: 0.09084796905517578\n",
      "Progress/train 82: 0/0 Loss: 0.04510130405426025, Time: 0.10167050361633301\n",
      "Progress/train 83: 0/0 Loss: 0.04431283473968506, Time: 0.08027935028076172\n",
      "Progress/train 84: 0/0 Loss: 0.04360787868499756, Time: 0.1235201358795166\n",
      "Progress/train 85: 0/0 Loss: 0.04434280872344971, Time: 0.16701459884643555\n",
      "Progress/train 86: 0/0 Loss: 0.04362128734588623, Time: 0.11403226852416992\n",
      "Progress/train 87: 0/0 Loss: 0.045941739082336425, Time: 0.07513999938964844\n",
      "Progress/train 88: 0/0 Loss: 0.04339873313903809, Time: 0.07561182975769043\n",
      "Progress/train 89: 0/0 Loss: 0.04391148567199707, Time: 0.07691597938537598\n",
      "Progress/train 90: 0/0 Loss: 0.04406402587890625, Time: 0.0829477310180664\n",
      "Progress/train 91: 0/0 Loss: 0.04403547286987305, Time: 0.07536578178405762\n",
      "Progress/train 92: 0/0 Loss: 0.04559487819671631, Time: 0.07932424545288086\n",
      "Progress/train 93: 0/0 Loss: 0.04353384494781494, Time: 0.06613993644714355\n",
      "Progress/train 94: 0/0 Loss: 0.04443389892578125, Time: 0.07134413719177246\n",
      "Progress/train 95: 0/0 Loss: 0.04486928462982178, Time: 0.07110285758972168\n",
      "Progress/train 96: 0/0 Loss: 0.04417166709899902, Time: 0.07497215270996094\n",
      "Progress/train 97: 0/0 Loss: 0.04420288562774658, Time: 0.06433367729187012\n",
      "Progress/train 98: 0/0 Loss: 0.0432830810546875, Time: 0.0824275016784668\n",
      "Progress/train 99: 0/0 Loss: 0.04359526634216308, Time: 0.07883214950561523\n",
      "Progress/train 100: 0/0 Loss: 0.04325803756713867, Time: 0.07106804847717285\n",
      "Progress/train 101: 0/0 Loss: 0.042425498962402344, Time: 0.07631778717041016\n",
      "Progress/train 102: 0/0 Loss: 0.044267678260803224, Time: 0.06973910331726074\n",
      "Progress/train 103: 0/0 Loss: 0.043092842102050784, Time: 0.07084226608276367\n",
      "Progress/train 104: 0/0 Loss: 0.04313826084136963, Time: 0.06893277168273926\n",
      "Progress/train 105: 0/0 Loss: 0.043146605491638186, Time: 0.08018064498901367\n",
      "Progress/train 106: 0/0 Loss: 0.04243249893188476, Time: 0.06982207298278809\n",
      "Progress/train 107: 0/0 Loss: 0.045151076316833495, Time: 0.07496213912963867\n",
      "Progress/train 108: 0/0 Loss: 0.04390305995941162, Time: 0.0708320140838623\n",
      "Progress/train 109: 0/0 Loss: 0.042301812171936036, Time: 0.0722501277923584\n",
      "Progress/train 110: 0/0 Loss: 0.04336596965789795, Time: 0.07932257652282715\n",
      "Progress/train 111: 0/0 Loss: 0.04446982383728027, Time: 0.07148313522338867\n",
      "Progress/train 112: 0/0 Loss: 0.04458614826202392, Time: 0.08362889289855957\n",
      "Progress/train 113: 0/0 Loss: 0.042060751914978024, Time: 0.08519887924194336\n",
      "Progress/train 114: 0/0 Loss: 0.04252479076385498, Time: 0.07813549041748047\n",
      "Progress/train 115: 0/0 Loss: 0.04421621322631836, Time: 0.0770728588104248\n",
      "Progress/train 116: 0/0 Loss: 0.04289928436279297, Time: 0.08505558967590332\n",
      "Progress/train 117: 0/0 Loss: 0.042739849090576175, Time: 0.07097268104553223\n",
      "Progress/train 118: 0/0 Loss: 0.04311428070068359, Time: 0.07231640815734863\n",
      "Progress/train 119: 0/0 Loss: 0.042183032035827635, Time: 0.07084441184997559\n",
      "Progress/train 120: 0/0 Loss: 0.04208714008331299, Time: 0.07726478576660156\n",
      "Progress/train 121: 0/0 Loss: 0.04241585731506348, Time: 0.07060408592224121\n",
      "Progress/train 122: 0/0 Loss: 0.04380685806274414, Time: 0.07704734802246094\n",
      "Progress/train 123: 0/0 Loss: 0.042307658195495604, Time: 0.07957077026367188\n",
      "Progress/train 124: 0/0 Loss: 0.04308337211608887, Time: 0.07061076164245605\n",
      "Progress/train 125: 0/0 Loss: 0.041668567657470706, Time: 0.07366275787353516\n",
      "Progress/train 126: 0/0 Loss: 0.04205082416534424, Time: 0.07830095291137695\n",
      "Progress/train 127: 0/0 Loss: 0.04163891792297363, Time: 0.07639908790588379\n",
      "Progress/train 128: 0/0 Loss: 0.04158860683441162, Time: 0.07359099388122559\n",
      "Progress/train 129: 0/0 Loss: 0.04238404750823974, Time: 0.07909202575683594\n",
      "Progress/train 130: 0/0 Loss: 0.04126779556274414, Time: 0.08008956909179688\n",
      "Progress/train 131: 0/0 Loss: 0.041332736015319824, Time: 0.11581802368164062\n",
      "Progress/train 132: 0/0 Loss: 0.041513986587524414, Time: 0.1132361888885498\n",
      "Progress/train 133: 0/0 Loss: 0.04262166023254395, Time: 0.08289003372192383\n",
      "Progress/train 134: 0/0 Loss: 0.0401268196105957, Time: 0.10181713104248047\n",
      "Progress/train 135: 0/0 Loss: 0.04260096549987793, Time: 0.06880617141723633\n",
      "Progress/train 136: 0/0 Loss: 0.042668538093566896, Time: 0.08000516891479492\n",
      "Progress/train 137: 0/0 Loss: 0.04192556381225586, Time: 0.08423042297363281\n",
      "Progress/train 138: 0/0 Loss: 0.04062618255615234, Time: 0.07951974868774414\n",
      "Progress/train 139: 0/0 Loss: 0.040355653762817384, Time: 0.07657313346862793\n",
      "Progress/train 140: 0/0 Loss: 0.040749979019165036, Time: 0.08602190017700195\n",
      "Progress/train 141: 0/0 Loss: 0.04156829833984375, Time: 0.10488104820251465\n",
      "Progress/train 142: 0/0 Loss: 0.04254055976867676, Time: 0.1145775318145752\n",
      "Progress/train 143: 0/0 Loss: 0.040731263160705564, Time: 0.15880179405212402\n",
      "Progress/train 144: 0/0 Loss: 0.04101193904876709, Time: 0.10376882553100586\n",
      "Progress/train 145: 0/0 Loss: 0.04329909324645996, Time: 0.1153404712677002\n",
      "Progress/train 146: 0/0 Loss: 0.04161757946014404, Time: 0.0976724624633789\n",
      "Progress/train 147: 0/0 Loss: 0.041399974822998044, Time: 0.0862739086151123\n",
      "Progress/train 148: 0/0 Loss: 0.042003087997436524, Time: 0.08972311019897461\n",
      "Progress/train 149: 0/0 Loss: 0.041141238212585446, Time: 0.09427356719970703\n",
      "Progress/train 150: 0/0 Loss: 0.04062985897064209, Time: 0.08250951766967773\n",
      "Progress/train 151: 0/0 Loss: 0.03978178024291992, Time: 0.08280396461486816\n",
      "Progress/train 152: 0/0 Loss: 0.04109277725219727, Time: 0.08353400230407715\n",
      "Progress/train 153: 0/0 Loss: 0.04025242328643799, Time: 0.09045267105102539\n",
      "Progress/train 154: 0/0 Loss: 0.040512433052062986, Time: 0.07962989807128906\n",
      "Progress/train 155: 0/0 Loss: 0.041142420768737795, Time: 0.07423758506774902\n",
      "Progress/train 156: 0/0 Loss: 0.040195035934448245, Time: 0.0731806755065918\n",
      "Progress/train 157: 0/0 Loss: 0.040777215957641604, Time: 0.09165525436401367\n",
      "Progress/train 158: 0/0 Loss: 0.04134110450744629, Time: 0.08666253089904785\n",
      "Progress/train 159: 0/0 Loss: 0.041883015632629396, Time: 0.07771825790405273\n",
      "Progress/train 160: 0/0 Loss: 0.03980238199234009, Time: 0.07018446922302246\n",
      "Progress/train 161: 0/0 Loss: 0.0403973913192749, Time: 0.0765993595123291\n",
      "Progress/train 162: 0/0 Loss: 0.040275249481201175, Time: 0.07007122039794922\n",
      "Progress/train 163: 0/0 Loss: 0.03979053258895874, Time: 0.09120392799377441\n",
      "Progress/train 164: 0/0 Loss: 0.040210328102111816, Time: 0.08382868766784668\n",
      "Progress/train 165: 0/0 Loss: 0.04017820835113525, Time: 0.08348631858825684\n",
      "Progress/train 166: 0/0 Loss: 0.040571861267089844, Time: 0.08093738555908203\n",
      "Progress/train 167: 0/0 Loss: 0.04094727039337158, Time: 0.08159637451171875\n",
      "Progress/train 168: 0/0 Loss: 0.04012411117553711, Time: 0.08152985572814941\n",
      "Progress/train 169: 0/0 Loss: 0.039638423919677736, Time: 0.08884215354919434\n",
      "Progress/train 170: 0/0 Loss: 0.04056140899658203, Time: 0.08609580993652344\n",
      "Progress/train 171: 0/0 Loss: 0.040520310401916504, Time: 0.08128786087036133\n",
      "Progress/train 172: 0/0 Loss: 0.03980215072631836, Time: 0.07541227340698242\n",
      "Progress/train 173: 0/0 Loss: 0.03838582992553711, Time: 0.07195234298706055\n",
      "Progress/train 174: 0/0 Loss: 0.04072658538818359, Time: 0.06635427474975586\n",
      "Progress/train 175: 0/0 Loss: 0.040501246452331545, Time: 0.07933521270751953\n",
      "Progress/train 176: 0/0 Loss: 0.040239567756652835, Time: 0.06896233558654785\n",
      "Progress/train 177: 0/0 Loss: 0.04046530723571777, Time: 0.07354617118835449\n",
      "Progress/train 178: 0/0 Loss: 0.03996843576431274, Time: 0.07316064834594727\n",
      "Progress/train 179: 0/0 Loss: 0.03939216136932373, Time: 0.07425427436828613\n",
      "Progress/train 180: 0/0 Loss: 0.04056990146636963, Time: 0.07123780250549316\n",
      "Progress/train 181: 0/0 Loss: 0.04065884590148926, Time: 0.07079410552978516\n",
      "Progress/train 182: 0/0 Loss: 0.03898164272308349, Time: 0.08511114120483398\n",
      "Progress/train 183: 0/0 Loss: 0.03873159646987915, Time: 0.07630443572998047\n",
      "Progress/train 184: 0/0 Loss: 0.040186681747436524, Time: 0.07286882400512695\n",
      "Progress/train 185: 0/0 Loss: 0.04157550811767578, Time: 0.07881355285644531\n",
      "Progress/train 186: 0/0 Loss: 0.038503656387329104, Time: 0.0741434097290039\n",
      "Progress/train 187: 0/0 Loss: 0.03836414098739624, Time: 0.07242918014526367\n",
      "Progress/train 188: 0/0 Loss: 0.03908077478408813, Time: 0.08428835868835449\n",
      "Progress/train 189: 0/0 Loss: 0.03860795497894287, Time: 0.07531070709228516\n",
      "Progress/train 190: 0/0 Loss: 0.03959376335144043, Time: 0.0834801197052002\n",
      "Progress/train 191: 0/0 Loss: 0.03920970439910889, Time: 0.08689498901367188\n",
      "Progress/train 192: 0/0 Loss: 0.03942137002944946, Time: 0.09409976005554199\n",
      "Progress/train 193: 0/0 Loss: 0.03964731693267822, Time: 0.07787799835205078\n",
      "Progress/train 194: 0/0 Loss: 0.039122321605682374, Time: 0.0868985652923584\n",
      "Progress/train 195: 0/0 Loss: 0.03990178108215332, Time: 0.08574485778808594\n",
      "Progress/train 196: 0/0 Loss: 0.03947459936141968, Time: 0.09333014488220215\n",
      "Progress/train 197: 0/0 Loss: 0.03819162368774414, Time: 0.09951090812683105\n",
      "Progress/train 198: 0/0 Loss: 0.04033858299255371, Time: 0.08062434196472168\n",
      "Progress/train 199: 0/0 Loss: 0.03858169078826904, Time: 0.09016180038452148\n",
      "Progress/train 200: 0/0 Loss: 0.03927117109298706, Time: 0.09387326240539551\n",
      "Progress/train 201: 0/0 Loss: 0.039870731830596924, Time: 0.06877422332763672\n",
      "Progress/train 202: 0/0 Loss: 0.03849756240844727, Time: 0.07680630683898926\n",
      "Progress/train 203: 0/0 Loss: 0.038874614238739016, Time: 0.07639575004577637\n",
      "Progress/train 204: 0/0 Loss: 0.0398797082901001, Time: 0.0899202823638916\n",
      "Progress/train 205: 0/0 Loss: 0.03850271701812744, Time: 0.07320857048034668\n",
      "Progress/train 206: 0/0 Loss: 0.03891687870025635, Time: 0.07630372047424316\n",
      "Progress/train 207: 0/0 Loss: 0.04032739639282226, Time: 0.0848541259765625\n",
      "Progress/train 208: 0/0 Loss: 0.03762464046478271, Time: 0.07074880599975586\n",
      "Progress/train 209: 0/0 Loss: 0.038398263454437254, Time: 0.07614326477050781\n",
      "Progress/train 210: 0/0 Loss: 0.03890204906463623, Time: 0.08270859718322754\n",
      "Progress/train 211: 0/0 Loss: 0.03847677230834961, Time: 0.06469845771789551\n",
      "Progress/train 212: 0/0 Loss: 0.03868240356445313, Time: 0.07351565361022949\n",
      "Progress/train 213: 0/0 Loss: 0.039023847579956056, Time: 0.07978463172912598\n",
      "Progress/train 214: 0/0 Loss: 0.03743411302566528, Time: 0.06322407722473145\n",
      "Progress/train 215: 0/0 Loss: 0.038498759269714355, Time: 0.07422757148742676\n",
      "Progress/train 216: 0/0 Loss: 0.03829967975616455, Time: 0.07099390029907227\n",
      "Progress/train 217: 0/0 Loss: 0.03862149238586426, Time: 0.07268285751342773\n",
      "Progress/train 218: 0/0 Loss: 0.03820324182510376, Time: 0.06611418724060059\n",
      "Progress/train 219: 0/0 Loss: 0.039042367935180664, Time: 0.07098579406738281\n",
      "Progress/train 220: 0/0 Loss: 0.037944674491882324, Time: 0.06603050231933594\n",
      "Progress/train 221: 0/0 Loss: 0.03826442956924438, Time: 0.060353994369506836\n",
      "Progress/train 222: 0/0 Loss: 0.03745044231414795, Time: 0.07610535621643066\n",
      "Progress/train 223: 0/0 Loss: 0.03735955715179443, Time: 0.07853817939758301\n",
      "Progress/train 224: 0/0 Loss: 0.038326518535614015, Time: 0.07948660850524902\n",
      "Progress/train 225: 0/0 Loss: 0.037152676582336425, Time: 0.07788848876953125\n",
      "Progress/train 226: 0/0 Loss: 0.03806996583938599, Time: 0.07017970085144043\n",
      "Progress/train 227: 0/0 Loss: 0.03786642551422119, Time: 0.07513999938964844\n",
      "Progress/train 228: 0/0 Loss: 0.03726095199584961, Time: 0.07804346084594727\n",
      "Progress/train 229: 0/0 Loss: 0.03875420093536377, Time: 0.06974148750305176\n",
      "Progress/train 230: 0/0 Loss: 0.03637525796890259, Time: 0.06449103355407715\n",
      "Progress/train 231: 0/0 Loss: 0.0367531156539917, Time: 0.07702898979187012\n",
      "Progress/train 232: 0/0 Loss: 0.0385992169380188, Time: 0.07652544975280762\n",
      "Progress/train 233: 0/0 Loss: 0.0377399468421936, Time: 0.06633615493774414\n",
      "Progress/train 234: 0/0 Loss: 0.03821467161178589, Time: 0.08367061614990234\n",
      "Progress/train 235: 0/0 Loss: 0.03824994802474976, Time: 0.05940103530883789\n",
      "Progress/train 236: 0/0 Loss: 0.038665320873260495, Time: 0.07441949844360352\n",
      "Progress/train 237: 0/0 Loss: 0.03903724193572998, Time: 0.07459568977355957\n",
      "Progress/train 238: 0/0 Loss: 0.03654810905456543, Time: 0.06815934181213379\n",
      "Progress/train 239: 0/0 Loss: 0.03734602928161621, Time: 0.07134413719177246\n",
      "Progress/train 240: 0/0 Loss: 0.037252221107482913, Time: 0.0703287124633789\n",
      "Progress/train 241: 0/0 Loss: 0.03791177749633789, Time: 0.07146120071411133\n",
      "Progress/train 242: 0/0 Loss: 0.03803129434585571, Time: 0.0639951229095459\n",
      "Progress/train 243: 0/0 Loss: 0.037696268558502194, Time: 0.07353615760803223\n",
      "Progress/train 244: 0/0 Loss: 0.0368436336517334, Time: 0.07314944267272949\n",
      "Progress/train 245: 0/0 Loss: 0.03789597988128662, Time: 0.06748104095458984\n",
      "Progress/train 246: 0/0 Loss: 0.037476205825805665, Time: 0.06311202049255371\n",
      "Progress/train 247: 0/0 Loss: 0.03798929929733277, Time: 0.07523846626281738\n",
      "Progress/train 248: 0/0 Loss: 0.03689101219177246, Time: 0.08027839660644531\n",
      "Progress/train 249: 0/0 Loss: 0.03634858846664429, Time: 0.08068370819091797\n",
      "Progress/train 250: 0/0 Loss: 0.03776605129241943, Time: 0.07372856140136719\n",
      "Progress/train 251: 0/0 Loss: 0.03696683645248413, Time: 0.07257318496704102\n",
      "Progress/train 252: 0/0 Loss: 0.03815083026885986, Time: 0.07981157302856445\n",
      "Progress/train 253: 0/0 Loss: 0.03650840997695923, Time: 0.0752871036529541\n",
      "Progress/train 254: 0/0 Loss: 0.0368381667137146, Time: 0.06209087371826172\n",
      "Progress/train 255: 0/0 Loss: 0.038283421993255615, Time: 0.07690048217773438\n",
      "Progress/train 256: 0/0 Loss: 0.036181437969207766, Time: 0.07735562324523926\n",
      "Progress/train 257: 0/0 Loss: 0.036731209754943844, Time: 0.08217525482177734\n",
      "Progress/train 258: 0/0 Loss: 0.036424169540405275, Time: 0.09073019027709961\n",
      "Progress/train 259: 0/0 Loss: 0.036943283081054684, Time: 0.08096909523010254\n",
      "Progress/train 260: 0/0 Loss: 0.03763489723205567, Time: 0.08531475067138672\n",
      "Progress/train 261: 0/0 Loss: 0.036648430824279786, Time: 0.09651064872741699\n",
      "Progress/train 262: 0/0 Loss: 0.03635798454284668, Time: 0.08239579200744629\n",
      "Progress/train 263: 0/0 Loss: 0.03715602397918701, Time: 0.1285257339477539\n",
      "Progress/train 264: 0/0 Loss: 0.037640714645385744, Time: 0.10226845741271973\n",
      "Progress/train 265: 0/0 Loss: 0.03727935314178467, Time: 0.09498405456542969\n",
      "Progress/train 266: 0/0 Loss: 0.03655901670455933, Time: 0.10426020622253418\n",
      "Progress/train 267: 0/0 Loss: 0.03788415670394898, Time: 0.07249665260314941\n",
      "Progress/train 268: 0/0 Loss: 0.03621042490005493, Time: 0.0784151554107666\n",
      "Progress/train 269: 0/0 Loss: 0.03643488168716431, Time: 0.07665681838989258\n",
      "Progress/train 270: 0/0 Loss: 0.037070794105529783, Time: 0.10681438446044922\n",
      "Progress/train 271: 0/0 Loss: 0.03719654321670532, Time: 0.1444408893585205\n",
      "Progress/train 272: 0/0 Loss: 0.035515117645263675, Time: 0.10713815689086914\n",
      "Progress/train 273: 0/0 Loss: 0.036232664585113525, Time: 0.1259901523590088\n",
      "Progress/train 274: 0/0 Loss: 0.035843961238861084, Time: 0.17037057876586914\n",
      "Progress/train 275: 0/0 Loss: 0.036953871250152585, Time: 0.10040640830993652\n",
      "Progress/train 276: 0/0 Loss: 0.03537451028823853, Time: 0.12884855270385742\n",
      "Progress/train 277: 0/0 Loss: 0.036593327522277834, Time: 0.07221198081970215\n",
      "Progress/train 278: 0/0 Loss: 0.036581621170043946, Time: 0.06213688850402832\n",
      "Progress/train 279: 0/0 Loss: 0.03671368837356567, Time: 0.07783675193786621\n",
      "Progress/train 280: 0/0 Loss: 0.03669514656066895, Time: 0.06866335868835449\n",
      "Progress/train 281: 0/0 Loss: 0.03633295297622681, Time: 0.07058095932006836\n",
      "Progress/train 282: 0/0 Loss: 0.03682957410812378, Time: 0.07114577293395996\n",
      "Progress/train 283: 0/0 Loss: 0.035628178119659425, Time: 0.08902215957641602\n",
      "Progress/train 284: 0/0 Loss: 0.03754791259765625, Time: 0.15401697158813477\n",
      "Progress/train 285: 0/0 Loss: 0.0363566517829895, Time: 0.08547854423522949\n",
      "Progress/train 286: 0/0 Loss: 0.03592287540435791, Time: 0.08679080009460449\n",
      "Progress/train 287: 0/0 Loss: 0.03620009660720825, Time: 0.07253003120422363\n",
      "Progress/train 288: 0/0 Loss: 0.035180327892303465, Time: 0.09374547004699707\n",
      "Progress/train 289: 0/0 Loss: 0.03652051448822022, Time: 0.07804012298583984\n",
      "Progress/train 290: 0/0 Loss: 0.03610676765441895, Time: 0.06804060935974121\n",
      "Progress/train 291: 0/0 Loss: 0.03648393869400025, Time: 0.08793807029724121\n",
      "Progress/train 292: 0/0 Loss: 0.03667562007904053, Time: 0.07646322250366211\n",
      "Progress/train 293: 0/0 Loss: 0.03577283382415772, Time: 0.0732889175415039\n",
      "Progress/train 294: 0/0 Loss: 0.035566821098327636, Time: 0.06977176666259766\n",
      "Progress/train 295: 0/0 Loss: 0.037098615169525145, Time: 0.07755398750305176\n",
      "Progress/train 296: 0/0 Loss: 0.036445739269256594, Time: 0.08209896087646484\n",
      "Progress/train 297: 0/0 Loss: 0.03559291124343872, Time: 0.10985255241394043\n",
      "Progress/train 298: 0/0 Loss: 0.03532657384872437, Time: 0.12625837326049805\n",
      "Progress/train 299: 0/0 Loss: 0.035085065364837645, Time: 0.18288326263427734\n",
      "Progress/train 300: 0/0 Loss: 0.036040821075439454, Time: 0.15616130828857422\n",
      "Progress/train 301: 0/0 Loss: 0.034448506832122805, Time: 0.1232903003692627\n",
      "Progress/train 302: 0/0 Loss: 0.03563514947891235, Time: 0.11727571487426758\n",
      "Progress/train 303: 0/0 Loss: 0.03550967216491699, Time: 0.14475178718566895\n",
      "Progress/train 304: 0/0 Loss: 0.036682579517364505, Time: 0.2538881301879883\n",
      "Progress/train 305: 0/0 Loss: 0.03558544397354126, Time: 0.14998078346252441\n",
      "Progress/train 306: 0/0 Loss: 0.03524523973464966, Time: 0.09726142883300781\n",
      "Progress/train 307: 0/0 Loss: 0.03601693868637085, Time: 0.2492058277130127\n",
      "Progress/train 308: 0/0 Loss: 0.034556035995483396, Time: 0.37041330337524414\n",
      "Progress/train 309: 0/0 Loss: 0.035957639217376706, Time: 0.10122442245483398\n",
      "Progress/train 310: 0/0 Loss: 0.03472734212875366, Time: 0.33421874046325684\n",
      "Progress/train 311: 0/0 Loss: 0.03564504861831665, Time: 0.3026926517486572\n",
      "Progress/train 312: 0/0 Loss: 0.03529434680938721, Time: 0.4499552249908447\n",
      "Progress/train 313: 0/0 Loss: 0.035212972164154054, Time: 0.20006942749023438\n",
      "Progress/train 314: 0/0 Loss: 0.03524679183959961, Time: 0.22289562225341797\n",
      "Progress/train 315: 0/0 Loss: 0.03547565698623657, Time: 0.2661745548248291\n",
      "Progress/train 316: 0/0 Loss: 0.035546579360961915, Time: 0.2845172882080078\n",
      "Progress/train 317: 0/0 Loss: 0.03427068710327148, Time: 0.27078914642333984\n",
      "Progress/train 318: 0/0 Loss: 0.034736709594726564, Time: 0.1743924617767334\n",
      "Progress/train 319: 0/0 Loss: 0.03526867389678955, Time: 0.14710402488708496\n",
      "Progress/train 320: 0/0 Loss: 0.03501043796539307, Time: 0.10774874687194824\n",
      "Progress/train 321: 0/0 Loss: 0.0355309510231018, Time: 0.10729670524597168\n",
      "Progress/train 322: 0/0 Loss: 0.03466845035552978, Time: 0.11550688743591309\n",
      "Progress/train 323: 0/0 Loss: 0.03496333599090576, Time: 0.12791109085083008\n",
      "Progress/train 324: 0/0 Loss: 0.03528379678726196, Time: 0.1463451385498047\n",
      "Progress/train 325: 0/0 Loss: 0.035507843494415284, Time: 0.10317087173461914\n",
      "Progress/train 326: 0/0 Loss: 0.0343553900718689, Time: 0.08804607391357422\n",
      "Progress/train 327: 0/0 Loss: 0.03495319604873657, Time: 0.09865188598632812\n",
      "Progress/train 328: 0/0 Loss: 0.03533313512802124, Time: 0.14272546768188477\n",
      "Progress/train 329: 0/0 Loss: 0.03616032838821411, Time: 0.08579182624816895\n",
      "Progress/train 330: 0/0 Loss: 0.0342221999168396, Time: 0.09139466285705566\n",
      "Progress/train 331: 0/0 Loss: 0.03574707984924316, Time: 0.10314822196960449\n",
      "Progress/train 332: 0/0 Loss: 0.03509782314300537, Time: 0.07408952713012695\n",
      "Progress/train 333: 0/0 Loss: 0.03480990648269653, Time: 0.0776667594909668\n",
      "Progress/train 334: 0/0 Loss: 0.033294458389282224, Time: 0.11289072036743164\n",
      "Progress/train 335: 0/0 Loss: 0.034669198989868165, Time: 0.1798086166381836\n",
      "Progress/train 336: 0/0 Loss: 0.0361001181602478, Time: 0.1907041072845459\n",
      "Progress/train 337: 0/0 Loss: 0.03402888774871826, Time: 0.13701653480529785\n",
      "Progress/train 338: 0/0 Loss: 0.034441041946411136, Time: 0.15516400337219238\n",
      "Progress/train 339: 0/0 Loss: 0.034014801979064944, Time: 0.1096503734588623\n",
      "Progress/train 340: 0/0 Loss: 0.034853713512420656, Time: 0.13865375518798828\n",
      "Progress/train 341: 0/0 Loss: 0.034417383670806885, Time: 0.11642217636108398\n",
      "Progress/train 342: 0/0 Loss: 0.03362065315246582, Time: 0.14129042625427246\n",
      "Progress/train 343: 0/0 Loss: 0.03359401226043701, Time: 0.21261811256408691\n",
      "Progress/train 344: 0/0 Loss: 0.03425671815872192, Time: 0.2342977523803711\n",
      "Progress/train 345: 0/0 Loss: 0.03383925676345825, Time: 0.20953702926635742\n",
      "Progress/train 346: 0/0 Loss: 0.03479432106018066, Time: 0.23943328857421875\n",
      "Progress/train 347: 0/0 Loss: 0.03430396556854248, Time: 0.2078113555908203\n",
      "Progress/train 348: 0/0 Loss: 0.03441401720046997, Time: 0.16678619384765625\n",
      "Progress/train 349: 0/0 Loss: 0.033995022773742674, Time: 0.22215986251831055\n",
      "Progress/train 350: 0/0 Loss: 0.0359119963645935, Time: 0.17325043678283691\n",
      "Progress/train 351: 0/0 Loss: 0.034610602855682376, Time: 0.15823984146118164\n",
      "Progress/train 352: 0/0 Loss: 0.03356768846511841, Time: 0.10879349708557129\n",
      "Progress/train 353: 0/0 Loss: 0.03443046808242798, Time: 0.0863645076751709\n",
      "Progress/train 354: 0/0 Loss: 0.03401863813400269, Time: 0.12528634071350098\n",
      "Progress/train 355: 0/0 Loss: 0.034885327816009525, Time: 0.11414480209350586\n",
      "Progress/train 356: 0/0 Loss: 0.03356168031692505, Time: 0.14592623710632324\n",
      "Progress/train 357: 0/0 Loss: 0.03293841361999512, Time: 0.09390902519226074\n",
      "Progress/train 358: 0/0 Loss: 0.0339876651763916, Time: 0.10055804252624512\n",
      "Progress/train 359: 0/0 Loss: 0.034028074741363525, Time: 0.13215065002441406\n",
      "Progress/train 360: 0/0 Loss: 0.03475847482681274, Time: 0.09954547882080078\n",
      "Progress/train 361: 0/0 Loss: 0.03406593084335327, Time: 0.11137604713439941\n",
      "Progress/train 362: 0/0 Loss: 0.03297492504119873, Time: 0.10006976127624512\n",
      "Progress/train 363: 0/0 Loss: 0.0341805362701416, Time: 0.11292195320129395\n",
      "Progress/train 364: 0/0 Loss: 0.03420931816101074, Time: 0.0931539535522461\n",
      "Progress/train 365: 0/0 Loss: 0.03419739484786987, Time: 0.0769948959350586\n",
      "Progress/train 366: 0/0 Loss: 0.03422360897064209, Time: 0.09931612014770508\n",
      "Progress/train 367: 0/0 Loss: 0.03353367805480957, Time: 0.0784156322479248\n",
      "Progress/train 368: 0/0 Loss: 0.033051986694335934, Time: 0.0838325023651123\n",
      "Progress/train 369: 0/0 Loss: 0.03316530704498291, Time: 0.0801534652709961\n",
      "Progress/train 370: 0/0 Loss: 0.03353365659713745, Time: 0.09073400497436523\n",
      "Progress/train 371: 0/0 Loss: 0.03220754384994507, Time: 0.09574484825134277\n",
      "Progress/train 372: 0/0 Loss: 0.03327956438064575, Time: 0.11468315124511719\n",
      "Progress/train 373: 0/0 Loss: 0.03311473369598389, Time: 0.0744478702545166\n",
      "Progress/train 374: 0/0 Loss: 0.03339139938354492, Time: 0.0893545150756836\n",
      "Progress/train 375: 0/0 Loss: 0.033616619110107424, Time: 0.0993657112121582\n",
      "Progress/train 376: 0/0 Loss: 0.032073028087615967, Time: 0.07986116409301758\n",
      "Progress/train 377: 0/0 Loss: 0.03277746200561524, Time: 0.07119035720825195\n",
      "Progress/train 378: 0/0 Loss: 0.032632975578308104, Time: 0.07901430130004883\n",
      "Progress/train 379: 0/0 Loss: 0.03331568717956543, Time: 0.08602428436279297\n",
      "Progress/train 380: 0/0 Loss: 0.03205691337585449, Time: 0.08481907844543457\n",
      "Progress/train 381: 0/0 Loss: 0.034363327026367185, Time: 0.12037944793701172\n",
      "Progress/train 382: 0/0 Loss: 0.03177500486373901, Time: 0.0757596492767334\n",
      "Progress/train 383: 0/0 Loss: 0.03378347635269165, Time: 0.0727992057800293\n",
      "Progress/train 384: 0/0 Loss: 0.033188660144805905, Time: 0.07665228843688965\n",
      "Progress/train 385: 0/0 Loss: 0.03313163995742798, Time: 0.08109569549560547\n",
      "Progress/train 386: 0/0 Loss: 0.03215804576873779, Time: 0.10524606704711914\n",
      "Progress/train 387: 0/0 Loss: 0.032691450119018556, Time: 0.08540701866149902\n",
      "Progress/train 388: 0/0 Loss: 0.032961559295654294, Time: 0.07421255111694336\n",
      "Progress/train 389: 0/0 Loss: 0.031905803680419925, Time: 0.08534812927246094\n",
      "Progress/train 390: 0/0 Loss: 0.031406776905059816, Time: 0.07144451141357422\n",
      "Progress/train 391: 0/0 Loss: 0.0322877049446106, Time: 0.07746529579162598\n",
      "Progress/train 392: 0/0 Loss: 0.031594994068145754, Time: 0.10469770431518555\n",
      "Progress/train 393: 0/0 Loss: 0.032709078788757326, Time: 0.08774900436401367\n",
      "Progress/train 394: 0/0 Loss: 0.032705988883972165, Time: 0.07982611656188965\n",
      "Progress/train 395: 0/0 Loss: 0.03233980655670166, Time: 0.06877946853637695\n",
      "Progress/train 396: 0/0 Loss: 0.03190117120742798, Time: 0.07835674285888672\n",
      "Progress/train 397: 0/0 Loss: 0.03091609001159668, Time: 0.08257770538330078\n",
      "Progress/train 398: 0/0 Loss: 0.031713788509368894, Time: 0.07951569557189941\n",
      "Progress/train 399: 0/0 Loss: 0.033830151557922364, Time: 0.13088130950927734\n",
      "Progress/train 400: 0/0 Loss: 0.03167467355728149, Time: 0.07565426826477051\n",
      "Progress/train 401: 0/0 Loss: 0.03162015438079834, Time: 0.07273006439208984\n",
      "Progress/train 402: 0/0 Loss: 0.03114478588104248, Time: 0.10253024101257324\n",
      "Progress/train 403: 0/0 Loss: 0.03145627021789551, Time: 0.12241768836975098\n",
      "Progress/train 404: 0/0 Loss: 0.03180418491363526, Time: 0.1315326690673828\n",
      "Progress/train 405: 0/0 Loss: 0.03065368890762329, Time: 0.08168363571166992\n",
      "Progress/train 406: 0/0 Loss: 0.03193100690841675, Time: 0.08390283584594727\n",
      "Progress/train 407: 0/0 Loss: 0.03150958061218262, Time: 0.1166834831237793\n",
      "Progress/train 408: 0/0 Loss: 0.03215952157974243, Time: 0.11318087577819824\n",
      "Progress/train 409: 0/0 Loss: 0.03104125738143921, Time: 0.14972448348999023\n",
      "Progress/train 410: 0/0 Loss: 0.03188455820083618, Time: 0.10071277618408203\n",
      "Progress/train 411: 0/0 Loss: 0.03204890966415405, Time: 0.0868687629699707\n",
      "Progress/train 412: 0/0 Loss: 0.031356916427612305, Time: 0.11284279823303223\n",
      "Progress/train 413: 0/0 Loss: 0.03238693237304688, Time: 0.15549087524414062\n",
      "Progress/train 414: 0/0 Loss: 0.031239593029022218, Time: 0.12063241004943848\n",
      "Progress/train 415: 0/0 Loss: 0.030732221603393554, Time: 0.1643199920654297\n",
      "Progress/train 416: 0/0 Loss: 0.032300353050231934, Time: 0.0899960994720459\n",
      "Progress/train 417: 0/0 Loss: 0.029881317615509034, Time: 0.0757906436920166\n",
      "Progress/train 418: 0/0 Loss: 0.03149853706359863, Time: 0.08273434638977051\n",
      "Progress/train 419: 0/0 Loss: 0.031092193126678467, Time: 0.07948803901672363\n",
      "Progress/train 420: 0/0 Loss: 0.031014869213104247, Time: 0.08368062973022461\n",
      "Progress/train 421: 0/0 Loss: 0.030572218894958494, Time: 0.07159113883972168\n",
      "Progress/train 422: 0/0 Loss: 0.03025425910949707, Time: 0.07979154586791992\n",
      "Progress/train 423: 0/0 Loss: 0.03164289951324463, Time: 0.10421872138977051\n",
      "Progress/train 424: 0/0 Loss: 0.031021625995635987, Time: 0.07138609886169434\n",
      "Progress/train 425: 0/0 Loss: 0.030662119388580322, Time: 0.16280245780944824\n",
      "Progress/train 426: 0/0 Loss: 0.030523672103881835, Time: 0.09287500381469727\n",
      "Progress/train 427: 0/0 Loss: 0.031791586875915524, Time: 0.11393928527832031\n",
      "Progress/train 428: 0/0 Loss: 0.03156031608581543, Time: 0.10420107841491699\n",
      "Progress/train 429: 0/0 Loss: 0.030656564235687255, Time: 0.09419393539428711\n",
      "Progress/train 430: 0/0 Loss: 0.03138014078140259, Time: 0.0814676284790039\n",
      "Progress/train 431: 0/0 Loss: 0.03006542682647705, Time: 0.06646990776062012\n",
      "Progress/train 432: 0/0 Loss: 0.030687174797058105, Time: 0.07581448554992676\n",
      "Progress/train 433: 0/0 Loss: 0.03216712713241577, Time: 0.06749296188354492\n",
      "Progress/train 434: 0/0 Loss: 0.030343952178955077, Time: 0.07410836219787598\n",
      "Progress/train 435: 0/0 Loss: 0.03010937452316284, Time: 0.06631302833557129\n",
      "Progress/train 436: 0/0 Loss: 0.031191987991333006, Time: 0.06615376472473145\n",
      "Progress/train 437: 0/0 Loss: 0.030906083583831786, Time: 0.06972289085388184\n",
      "Progress/train 438: 0/0 Loss: 0.02963548421859741, Time: 0.06182980537414551\n",
      "Progress/train 439: 0/0 Loss: 0.030554895401000978, Time: 0.056757450103759766\n",
      "Progress/train 440: 0/0 Loss: 0.029572367668151855, Time: 0.0696268081665039\n",
      "Progress/train 441: 0/0 Loss: 0.02939180612564087, Time: 0.06099891662597656\n",
      "Progress/train 442: 0/0 Loss: 0.029213674068450927, Time: 0.05822157859802246\n",
      "Progress/train 443: 0/0 Loss: 0.03009253740310669, Time: 0.0560307502746582\n",
      "Progress/train 444: 0/0 Loss: 0.029954710006713868, Time: 0.07302713394165039\n",
      "Progress/train 445: 0/0 Loss: 0.030173180103302003, Time: 0.062299489974975586\n",
      "Progress/train 446: 0/0 Loss: 0.030726544857025147, Time: 0.05435371398925781\n",
      "Progress/train 447: 0/0 Loss: 0.030491085052490235, Time: 0.07069015502929688\n",
      "Progress/train 448: 0/0 Loss: 0.029940958023071288, Time: 0.06225419044494629\n",
      "Progress/train 449: 0/0 Loss: 0.030256426334381102, Time: 0.05687880516052246\n",
      "Progress/train 450: 0/0 Loss: 0.029316725730895995, Time: 0.0647726058959961\n",
      "Progress/train 451: 0/0 Loss: 0.02912179470062256, Time: 0.057349443435668945\n",
      "Progress/train 452: 0/0 Loss: 0.029560697078704835, Time: 0.06470346450805664\n",
      "Progress/train 453: 0/0 Loss: 0.029886667728424073, Time: 0.05829334259033203\n",
      "Progress/train 454: 0/0 Loss: 0.029760298728942872, Time: 0.054297447204589844\n",
      "Progress/train 455: 0/0 Loss: 0.029706499576568603, Time: 0.053662776947021484\n",
      "Progress/train 456: 0/0 Loss: 0.030440049171447756, Time: 0.0600285530090332\n",
      "Progress/train 457: 0/0 Loss: 0.02990675449371338, Time: 0.05849123001098633\n",
      "Progress/train 458: 0/0 Loss: 0.02928740978240967, Time: 0.06530499458312988\n",
      "Progress/train 459: 0/0 Loss: 0.029707770347595214, Time: 0.059302330017089844\n",
      "Progress/train 460: 0/0 Loss: 0.029149096012115478, Time: 0.0676882266998291\n",
      "Progress/train 461: 0/0 Loss: 0.028481199741363525, Time: 0.060858726501464844\n",
      "Progress/train 462: 0/0 Loss: 0.029155840873718263, Time: 0.052412986755371094\n",
      "Progress/train 463: 0/0 Loss: 0.02954787254333496, Time: 0.05503988265991211\n",
      "Progress/train 464: 0/0 Loss: 0.030159783363342286, Time: 0.06447196006774902\n",
      "Progress/train 465: 0/0 Loss: 0.02951680660247803, Time: 0.07636737823486328\n",
      "Progress/train 466: 0/0 Loss: 0.02918834924697876, Time: 0.06144523620605469\n",
      "Progress/train 467: 0/0 Loss: 0.028561553955078124, Time: 0.0630946159362793\n",
      "Progress/train 468: 0/0 Loss: 0.02897564888000488, Time: 0.07319831848144531\n",
      "Progress/train 469: 0/0 Loss: 0.027522120475769043, Time: 0.0723717212677002\n",
      "Progress/train 470: 0/0 Loss: 0.029050283432006836, Time: 0.08122038841247559\n",
      "Progress/train 471: 0/0 Loss: 0.028234379291534425, Time: 0.07630729675292969\n",
      "Progress/train 472: 0/0 Loss: 0.028189344406127928, Time: 0.0763094425201416\n",
      "Progress/train 473: 0/0 Loss: 0.028443818092346192, Time: 0.10797333717346191\n",
      "Progress/train 474: 0/0 Loss: 0.027236642837524413, Time: 0.06389093399047852\n",
      "Progress/train 475: 0/0 Loss: 0.027571985721588133, Time: 0.06307125091552734\n",
      "Progress/train 476: 0/0 Loss: 0.02927828311920166, Time: 0.05769658088684082\n",
      "Progress/train 477: 0/0 Loss: 0.028667147159576415, Time: 0.0809328556060791\n",
      "Progress/train 478: 0/0 Loss: 0.027387259006500245, Time: 0.0886991024017334\n",
      "Progress/train 479: 0/0 Loss: 0.02720855951309204, Time: 0.1052999496459961\n",
      "Progress/train 480: 0/0 Loss: 0.028441483974456786, Time: 0.10120058059692383\n",
      "Progress/train 481: 0/0 Loss: 0.02817354440689087, Time: 0.07033371925354004\n",
      "Progress/train 482: 0/0 Loss: 0.02775965929031372, Time: 0.10078763961791992\n",
      "Progress/train 483: 0/0 Loss: 0.027713282108306883, Time: 0.08093690872192383\n",
      "Progress/train 484: 0/0 Loss: 0.02733107089996338, Time: 0.07600283622741699\n",
      "Progress/train 485: 0/0 Loss: 0.028133704662322997, Time: 0.07655668258666992\n",
      "Progress/train 486: 0/0 Loss: 0.028321642875671387, Time: 0.08513021469116211\n",
      "Progress/train 487: 0/0 Loss: 0.027587976455688477, Time: 0.13743305206298828\n",
      "Progress/train 488: 0/0 Loss: 0.026826231479644774, Time: 0.12910103797912598\n",
      "Progress/train 489: 0/0 Loss: 0.02855837106704712, Time: 0.12249112129211426\n",
      "Progress/train 490: 0/0 Loss: 0.025908493995666505, Time: 0.16143465042114258\n",
      "Progress/train 491: 0/0 Loss: 0.026852500438690186, Time: 0.0726921558380127\n",
      "Progress/train 492: 0/0 Loss: 0.027433671951293946, Time: 0.0665891170501709\n",
      "Progress/train 493: 0/0 Loss: 0.026485321521759034, Time: 0.14361834526062012\n",
      "Progress/train 494: 0/0 Loss: 0.027842783927917482, Time: 0.09552502632141113\n",
      "Progress/train 495: 0/0 Loss: 0.027889788150787354, Time: 0.162247896194458\n",
      "Progress/train 496: 0/0 Loss: 0.02760192155838013, Time: 0.19100141525268555\n",
      "Progress/train 497: 0/0 Loss: 0.02782175540924072, Time: 0.16417598724365234\n",
      "Progress/train 498: 0/0 Loss: 0.027504262924194337, Time: 0.2566554546356201\n",
      "Progress/train 499: 0/0 Loss: 0.028001275062561035, Time: 0.7457280158996582\n"
     ]
    }
   ],
   "source": [
    "from model import WMTModel\n",
    "model = WMTModel(en_token.get_vocab_size(), de_token.get_vocab_size() , 64)\n",
    "# Set to square root of model\n",
    "# Then multiply by  min(step_num^0.5 , step_num * warmup_steps^1.5)\n",
    "initial_lr = 512 ** -0.5\n",
    "warmup_steps = 4000\n",
    "multiplier_lambda = lambda step: min((step+1) ** -0.5, (step+1) * warmup_steps ** -1.5)\n",
    "optimizer = torch.optim.Adam(model.parameters(), betas=(0.9, 0.98), eps=1e-9, lr=initial_lr)\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, multiplier_lambda)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "writer = SummaryWriter()\n",
    "for epoch in range(500):\n",
    "    train(model, optimizer, scheduler ,criterion, dataloader_train, writer, epoch, minibatch=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(model, dataloader_test, writer, de_token)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('NLP')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "47d1cf54bf9cf5fce4b000eacb105df7d7e5f1fe165267018e0a6855939e5736"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "YcP7qZGOA_4f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torchmetrics in /home/kydliceh/.conda/envs/NLP/lib/python3.10/site-packages (0.10.1)\n",
            "Requirement already satisfied: datasets in /home/kydliceh/.conda/envs/NLP/lib/python3.10/site-packages (2.8.0)\n",
            "Requirement already satisfied: tokenizers in /home/kydliceh/.conda/envs/NLP/lib/python3.10/site-packages (0.11.4)\n",
            "Requirement already satisfied: packaging in /home/kydliceh/.conda/envs/NLP/lib/python3.10/site-packages (from torchmetrics) (21.3)\n",
            "Requirement already satisfied: torch>=1.3.1 in /home/kydliceh/.conda/envs/NLP/lib/python3.10/site-packages (from torchmetrics) (1.12.1)\n",
            "Requirement already satisfied: numpy>=1.17.2 in /home/kydliceh/.local/lib/python3.10/site-packages (from torchmetrics) (1.23.3)\n",
            "Requirement already satisfied: pyarrow>=6.0.0 in /home/kydliceh/.conda/envs/NLP/lib/python3.10/site-packages (from datasets) (8.0.0)\n",
            "Requirement already satisfied: dill<0.3.7 in /home/kydliceh/.conda/envs/NLP/lib/python3.10/site-packages (from datasets) (0.3.6)\n",
            "Requirement already satisfied: pandas in /home/kydliceh/.local/lib/python3.10/site-packages (from datasets) (1.4.4)\n",
            "Requirement already satisfied: requests>=2.19.0 in /home/kydliceh/.conda/envs/NLP/lib/python3.10/site-packages (from datasets) (2.28.1)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /home/kydliceh/.conda/envs/NLP/lib/python3.10/site-packages (from datasets) (4.64.0)\n",
            "Requirement already satisfied: xxhash in /home/kydliceh/.conda/envs/NLP/lib/python3.10/site-packages (from datasets) (0.0.0)\n",
            "Requirement already satisfied: multiprocess in /home/kydliceh/.conda/envs/NLP/lib/python3.10/site-packages (from datasets) (0.70.12.2)\n",
            "Requirement already satisfied: fsspec[http]>=2021.11.1 in /home/kydliceh/.conda/envs/NLP/lib/python3.10/site-packages (from datasets) (2022.11.0)\n",
            "Requirement already satisfied: aiohttp in /home/kydliceh/.conda/envs/NLP/lib/python3.10/site-packages (from datasets) (3.8.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.2.0 in /home/kydliceh/.conda/envs/NLP/lib/python3.10/site-packages (from datasets) (0.11.1)\n",
            "Requirement already satisfied: responses<0.19 in /home/kydliceh/.conda/envs/NLP/lib/python3.10/site-packages (from datasets) (0.18.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /home/kydliceh/.conda/envs/NLP/lib/python3.10/site-packages (from datasets) (6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /home/kydliceh/.local/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /home/kydliceh/.local/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /home/kydliceh/.conda/envs/NLP/lib/python3.10/site-packages (from aiohttp->datasets) (1.8.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /home/kydliceh/.conda/envs/NLP/lib/python3.10/site-packages (from aiohttp->datasets) (21.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /home/kydliceh/.conda/envs/NLP/lib/python3.10/site-packages (from aiohttp->datasets) (1.2.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /home/kydliceh/.conda/envs/NLP/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.2)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /home/kydliceh/.local/lib/python3.10/site-packages (from aiohttp->datasets) (2.0.12)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/kydliceh/.local/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (4.4.0)\n",
            "Requirement already satisfied: filelock in /home/kydliceh/.conda/envs/NLP/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (3.8.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/kydliceh/.conda/envs/NLP/lib/python3.10/site-packages (from packaging->torchmetrics) (3.0.9)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /home/kydliceh/.conda/envs/NLP/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/kydliceh/.conda/envs/NLP/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (1.26.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /home/kydliceh/.conda/envs/NLP/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (2022.12.7)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /home/kydliceh/.local/lib/python3.10/site-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /home/kydliceh/.local/lib/python3.10/site-packages (from pandas->datasets) (2022.2.1)\n",
            "Requirement already satisfied: six>=1.5 in /home/kydliceh/.conda/envs/NLP/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install torchmetrics datasets tokenizers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "LI4lW4x9A_4i"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Found cached dataset wmt14 (/home/kydliceh/.cache/huggingface/datasets/wmt14/de-en/1.0.0/2de185b074515e97618524d69f5e27ee7545dcbed4aa9bc1a4235710ffca33f4)\n"
          ]
        },
        {
          "data": {
            "application/json": {
              "ascii": false,
              "bar_format": null,
              "colour": null,
              "elapsed": 0.030216693878173828,
              "initial": 0,
              "n": 0,
              "ncols": null,
              "nrows": null,
              "postfix": null,
              "prefix": "",
              "rate": null,
              "total": 3,
              "unit": "it",
              "unit_divisor": 1000,
              "unit_scale": false
            },
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d10e8bf106104d7191655d7e622e2e84",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "wmt14 = load_dataset('wmt14', 'de-en')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "DTHjttReBG5f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "fatal: destination path 'Statistical-learning-class' already exists and is not an empty directory.\n",
            "[Errno 2] No such file or directory: '/content/Statistical-learning-class/Assigments/Project'\n",
            "/home/kydliceh/Projects/R/Assigments/Project\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/hynky1999/Statistical-learning-class\n",
        "%cd /content/Statistical-learning-class/Assigments/Project"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "aD2yRsMyA_4j"
      },
      "outputs": [],
      "source": [
        "train_subset_length = 10\n",
        "test_subset_length = len(wmt14['test'])\n",
        "vocab_size=10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "yp9xKHWeA_4j"
      },
      "outputs": [],
      "source": [
        "train_dataset = wmt14['train'].select(range(train_subset_length))\n",
        "test_dataset = wmt14['test'].select(range(test_subset_length))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "jPGN6kzyA_4k"
      },
      "outputs": [],
      "source": [
        "from tokenizers import ByteLevelBPETokenizer, Tokenizer\n",
        "\n",
        "\n",
        "\n",
        "def create_tokenizer(iterable, add_special_tokens=False, train=True, vocab_size=1000):\n",
        "    toknizer = ByteLevelBPETokenizer()\n",
        "    if train:\n",
        "        toknizer.train_from_iterator(iterable, vocab_size=vocab_size, min_frequency=2, special_tokens=[\"[PAD]\"])\n",
        "\n",
        "    if add_special_tokens:\n",
        "        toknizer.add_special_tokens([\"[START]\", \"[END]\"])\n",
        "    return toknizer\n",
        "\n",
        "\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "GGPb-9rVA_4k"
      },
      "outputs": [],
      "source": [
        "\n",
        "de_it = map(lambda x: x['de'] , train_dataset['translation'])\n",
        "en_it = map(lambda x: x['en'] , train_dataset['translation'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "NqR6TTiXA_4k"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "de_token = create_tokenizer(de_it, train=True, vocab_size=vocab_size, add_special_tokens=True)\n",
        "en_token = create_tokenizer(en_it, train=True, vocab_size=vocab_size)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "de_token.save(\"de_token.json\")\n",
        "en_token.save(\"en_token.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "from tokenizers import ByteLevelBPETokenizer, Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "de_token = Tokenizer.from_file(\"de_token.json\")\n",
        "en_token = Tokenizer.from_file(\"en_token.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "'str' object cannot be interpreted as an integer",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn [27], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m de_token\u001b[39m.\u001b[39;49mdecode(de_token\u001b[39m.\u001b[39;49mencode(\u001b[39m\"\u001b[39;49m\u001b[39mHallo Welt\u001b[39;49m\u001b[39m\"\u001b[39;49m)\u001b[39m.\u001b[39;49mtokens)\n",
            "\u001b[0;31mTypeError\u001b[0m: 'str' object cannot be interpreted as an integer"
          ]
        }
      ],
      "source": [
        "de_token.decode(de_token.encode(\"Hallo Welt\").ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P8VMntg2A_4l"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mqtTUtjKA_4l"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "def extract_embedding(embeds, lang):\n",
        "    return {f\"{lang}_ids\": [e.ids for e in embeds], f\"{lang}_att\": [e.attention_mask for e in embeds]}\n",
        "\n",
        "def add_lenghts(trans):\n",
        "  translation = trans[\"translation\"]\n",
        "  de_sent = [t[\"de\"] for t in translation]\n",
        "  en_sent = [t[\"en\"] for t in translation]\n",
        "  en_len = [len(t[\"en\"]) for t in translation]\n",
        "  de_len = [len(t[\"de\"]) for t in translation]\n",
        "  return {\"en_len\": en_len, \"de_len\": de_len,\"de_sent\": de_sent, \"en_sent\": en_sent}\n",
        "\n",
        "\n",
        "\n",
        "def tokenize(trans):\n",
        "    translation = trans[\"translation\"]\n",
        "    de_sent = [t[\"de\"] for t in translation]\n",
        "    en_sent = [t[\"en\"] for t in translation]\n",
        "    en = en_token.encode_batch(en_sent)\n",
        "    de = de_token.encode_batch(de_sent)\n",
        "    dct = {**extract_embedding(en, \"en\"), **extract_embedding(de, \"de\")}\n",
        "    return dct\n",
        "\n",
        "def collate_fc(batch):\n",
        "    en_ids_padded = torch.nn.utils.rnn.pad_sequence([b[\"en_ids\"] for b in batch], batch_first=True, padding_value=en_token.token_to_id(\"[PAD]\"))\n",
        "\n",
        "    de_ids = [torch.cat([torch.tensor([de_token.token_to_id(\"[START]\")]), b[\"de_ids\"], torch.tensor([de_token.token_to_id(\"[END]\")])]) for b in batch]\n",
        "\n",
        "    de_ids_padded = torch.nn.utils.rnn.pad_sequence(de_ids, batch_first=True, padding_value=de_token.token_to_id(\"[PAD]\"))\n",
        "\n",
        "    en_sents = [b[\"en_sent\"] for b in batch]\n",
        "    de_sents = [b[\"de_sent\"] for b in batch]\n",
        "\n",
        "    return {\"en_ids\": en_ids_padded, \"de_ids\": de_ids_padded, \"de_sent\": de_sents, \"en_sent\": en_sents}\n",
        "\n",
        "def create_dataloader(dataset, batch_size=32, shuffle=False):\n",
        "    tokenized = dataset.map(add_lenghts, batch_size=batch_size, batched=True)\n",
        "    # Sort by lengths to get smaller paddings\n",
        "    tokenized = tokenized.sort(\"en_len\")\n",
        "    tokenized = tokenized.sort(\"de_len\",kind=\"stable\")\n",
        "    tokenized = tokenized.map(tokenize, batch_size=batch_size, batched=True)\n",
        "    tokenized = tokenized.remove_columns(\"translation\")\n",
        "    tokenized.set_format(\"torch\")\n",
        "    return DataLoader(tokenized, batch_size=batch_size, shuffle=True,collate_fn=collate_fc)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'train_dataset' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn [3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mnext\u001b[39m(\u001b[39miter\u001b[39m(create_dataloader(train_dataset, batch_size\u001b[39m=\u001b[39m\u001b[39m100\u001b[39m)))[\u001b[39m\"\u001b[39m\u001b[39men_ids\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mshape\n",
            "\u001b[0;31mNameError\u001b[0m: name 'train_dataset' is not defined"
          ]
        }
      ],
      "source": [
        "next(iter(create_dataloader(train_dataset, batch_size=100)))[\"en_ids\"].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Kz4NTnXA_4m"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading cached processed dataset at /home/kydliceh/.cache/huggingface/datasets/wmt14/de-en/1.0.0/2de185b074515e97618524d69f5e27ee7545dcbed4aa9bc1a4235710ffca33f4/cache-822b218e8b90543c.arrow\n",
            "Loading cached sorted indices for dataset at /home/kydliceh/.cache/huggingface/datasets/wmt14/de-en/1.0.0/2de185b074515e97618524d69f5e27ee7545dcbed4aa9bc1a4235710ffca33f4/cache-f6ac8a6d08304d29.arrow\n",
            "Loading cached sorted indices for dataset at /home/kydliceh/.cache/huggingface/datasets/wmt14/de-en/1.0.0/2de185b074515e97618524d69f5e27ee7545dcbed4aa9bc1a4235710ffca33f4/cache-70c0be8a84b650d4.arrow\n",
            "Loading cached processed dataset at /home/kydliceh/.cache/huggingface/datasets/wmt14/de-en/1.0.0/2de185b074515e97618524d69f5e27ee7545dcbed4aa9bc1a4235710ffca33f4/cache-cbed3d89d3c8f159.arrow\n"
          ]
        },
        {
          "data": {
            "application/json": {
              "ascii": false,
              "bar_format": null,
              "colour": null,
              "elapsed": 0.03270745277404785,
              "initial": 0,
              "n": 0,
              "ncols": null,
              "nrows": null,
              "postfix": null,
              "prefix": "",
              "rate": null,
              "total": 1000,
              "unit": "ba",
              "unit_divisor": 1000,
              "unit_scale": false
            },
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6b0b7873797a401bb4a83c3a1f680c78",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1000 [00:00<?, ?ba/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/json": {
              "ascii": false,
              "bar_format": null,
              "colour": null,
              "elapsed": 0.03358936309814453,
              "initial": 0,
              "n": 0,
              "ncols": null,
              "nrows": null,
              "postfix": null,
              "prefix": "",
              "rate": null,
              "total": 1000,
              "unit": "ba",
              "unit_divisor": 1000,
              "unit_scale": false
            },
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "86ac84c168c04d8ba61310d42baff93b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1000 [00:00<?, ?ba/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "dataloader_train = create_dataloader(train_dataset, batch_size=2, shuffle=True)\n",
        "dataloader_test = create_dataloader(test_dataset, batch_size=2, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8HPiuLLVA_4m"
      },
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-ecmFfF2A_4n"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oxwr40aGA_4n"
      },
      "outputs": [],
      "source": [
        "from train_test import train, evaluate\n",
        "import torch.nn as nn\n",
        "from torch.utils.tensorboard import SummaryWriter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DtocloMZA_4n"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Progress/train 0: 0/0 Loss: 5.643998622894287, Time: 0.39458537101745605\n",
            "Progress/train 1: 0/0 Loss: 5.6983184814453125, Time: 0.4585902690887451\n",
            "Progress/train 2: 0/0 Loss: 5.66721773147583, Time: 0.811173677444458\n",
            "Progress/train 3: 0/0 Loss: 5.664276123046875, Time: 0.9107768535614014\n",
            "Progress/train 4: 0/0 Loss: 5.620601654052734, Time: 0.39592552185058594\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Exception ignored in: <function Dataset.__del__ at 0x7f76907724d0>\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/kydliceh/.conda/envs/NLP/lib/python3.10/site-packages/datasets/arrow_dataset.py\", line 1248, in __del__\n",
            "    def __del__(self):\n",
            "  File \"_pydevd_bundle/pydevd_cython.pyx\", line 1443, in _pydevd_bundle.pydevd_cython.SafeCallWrapper.__call__\n",
            "  File \"_pydevd_bundle/pydevd_cython.pyx\", line 1744, in _pydevd_bundle.pydevd_cython.ThreadTracer.__call__\n",
            "  File \"/home/kydliceh/.local/lib/python3.10/site-packages/debugpy/_vendored/pydevd/_pydev_bundle/pydev_is_thread_alive.py\", line 9, in is_thread_alive\n",
            "    def is_thread_alive(t):\n",
            "KeyboardInterrupt: \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Progress/train 5: 0/0 Loss: 5.70603084564209, Time: 0.17795944213867188\n",
            "Progress/train 6: 0/0 Loss: 5.681488513946533, Time: 0.757025957107544\n",
            "Progress/train 7: 0/0 Loss: 5.744671821594238, Time: 0.19061541557312012\n",
            "Progress/train 8: 0/0 Loss: 5.645264148712158, Time: 0.4288811683654785\n",
            "Progress/train 9: 0/0 Loss: 5.626806259155273, Time: 0.6366860866546631\n",
            "Progress/train 10: 0/0 Loss: 5.624164581298828, Time: 0.8401386737823486\n",
            "Progress/train 11: 0/0 Loss: 5.6089019775390625, Time: 0.40249109268188477\n",
            "Progress/train 12: 0/0 Loss: 5.661619663238525, Time: 0.20670270919799805\n",
            "Progress/train 13: 0/0 Loss: 5.644193172454834, Time: 0.8352255821228027\n",
            "Progress/train 14: 0/0 Loss: 5.590433597564697, Time: 0.6516189575195312\n",
            "Progress/train 15: 0/0 Loss: 5.6176323890686035, Time: 0.6995832920074463\n",
            "Progress/train 16: 0/0 Loss: 5.635136127471924, Time: 0.3603208065032959\n",
            "Progress/train 17: 0/0 Loss: 5.705245494842529, Time: 0.3335585594177246\n",
            "Progress/train 18: 0/0 Loss: 5.57806396484375, Time: 0.8357033729553223\n",
            "Progress/train 19: 0/0 Loss: 5.649967670440674, Time: 0.21771025657653809\n",
            "Progress/train 20: 0/0 Loss: 5.534117698669434, Time: 0.860971212387085\n",
            "Progress/train 21: 0/0 Loss: 5.5730438232421875, Time: 0.3070497512817383\n",
            "Progress/train 22: 0/0 Loss: 5.535096168518066, Time: 0.6486432552337646\n",
            "Progress/train 23: 0/0 Loss: 5.468520641326904, Time: 0.5858721733093262\n",
            "Progress/train 24: 0/0 Loss: 5.514462471008301, Time: 0.4293861389160156\n"
          ]
        }
      ],
      "source": [
        "from model import WMTModel\n",
        "d_model=128\n",
        "model = WMTModel(en_token.get_vocab_size(), de_token.get_vocab_size() , d_model)\n",
        "model.to(device)\n",
        "# Set to square root of model\n",
        "# Then multiply by  min(step_num^−0.5 , step_num * warmup_steps^−1.5)\n",
        "initial_lr = d_model ** -0.5\n",
        "warmup_steps = 4000\n",
        "multiplier_lambda = lambda step: min((step+1) ** -0.5, (step+1) * warmup_steps ** -1.5)\n",
        "optimizer = torch.optim.Adam(model.parameters(), betas=(0.9, 0.98), eps=1e-9, lr=initial_lr)\n",
        "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, multiplier_lambda)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
        "writer = SummaryWriter()\n",
        "epochs=1000\n",
        "save_every=2\n",
        "for epoch in range(epochs):\n",
        "    train(model, optimizer, scheduler ,criterion, dataloader_train, writer, epoch, minibatch=True, pad_token=en_token.token_to_id(\"[PAD]\"))\n",
        "\n",
        "    if epoch % save_every == 0:\n",
        "        torch.save(model.state_dict(), f\"model_{epoch}.pt\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VjOgcFxmA_4u"
      },
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn [143], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m evaluate(model, dataloader_test, writer, de_token)\n",
            "File \u001b[0;32m~/Projects/R/Assigments/Project/train_test.py:111\u001b[0m, in \u001b[0;36mevaluate\u001b[0;34m(model, test_data, writer, de_tokenizer)\u001b[0m\n\u001b[1;32m    108\u001b[0m source_mask \u001b[39m=\u001b[39m Transformer\u001b[39m.\u001b[39mmake_src_mask(source_ids, pad_token)\n\u001b[1;32m    110\u001b[0m target_sent \u001b[39m=\u001b[39m batch[\u001b[39m\"\u001b[39m\u001b[39mde_sent\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m--> 111\u001b[0m predicted \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mpredict(source_ids, source_mask, start_token, end_token)\n\u001b[1;32m    112\u001b[0m \u001b[39m# Convertes predicted to list of sentences and then strips the start and end tokens\u001b[39;00m\n\u001b[1;32m    113\u001b[0m without_start_end \u001b[39m=\u001b[39m [\n\u001b[1;32m    114\u001b[0m     strip_start_end(p, end_token\u001b[39m=\u001b[39mend_token) \u001b[39mfor\u001b[39;00m p \u001b[39min\u001b[39;00m predicted\u001b[39m.\u001b[39mtolist()\n\u001b[1;32m    115\u001b[0m ]\n",
            "File \u001b[0;32m~/Projects/R/Assigments/Project/model.py:66\u001b[0m, in \u001b[0;36mWMTModel.predict\u001b[0;34m(self, src, src_mask, start_token, end_token, max_len)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(max_len):\n\u001b[1;32m     63\u001b[0m     trg_mask \u001b[39m=\u001b[39m Transformer\u001b[39m.\u001b[39mmake_trg_mask(\n\u001b[1;32m     64\u001b[0m         torch\u001b[39m.\u001b[39mones([trg\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], \u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m, trg\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]]) \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m     65\u001b[0m     )\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m---> 66\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransformer\u001b[39m.\u001b[39;49mdecode(trg_embedding, enc_output, trg_mask, src_mask)\n\u001b[1;32m     67\u001b[0m     pred \u001b[39m=\u001b[39m out[:, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39margmax(\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39munsqueeze(\u001b[39m1\u001b[39m)\n\u001b[1;32m     68\u001b[0m     pred_embedding \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpos_encoding(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mDE_embedding(pred))\n",
            "File \u001b[0;32m~/Projects/R/Assigments/Project/transfomer.py:262\u001b[0m, in \u001b[0;36mTransformer.decode\u001b[0;34m(self, dec_input, enc_input, trg_mask, enc_mask)\u001b[0m\n\u001b[1;32m    261\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecode\u001b[39m(\u001b[39mself\u001b[39m, dec_input, enc_input, trg_mask, enc_mask):\n\u001b[0;32m--> 262\u001b[0m     dec_out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdec(dec_input, enc_input, trg_mask, enc_mask)\n\u001b[1;32m    263\u001b[0m     dec_out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrg_linear(dec_out)\n\u001b[1;32m    264\u001b[0m     \u001b[39mreturn\u001b[39;00m dec_out\n",
            "File \u001b[0;32m~/.conda/envs/NLP/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[0;32m~/Projects/R/Assigments/Project/transfomer.py:194\u001b[0m, in \u001b[0;36mTransfomerDecoder.forward\u001b[0;34m(self, x, enc_out, self_mask, enc_mask)\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x, enc_out, self_mask, enc_mask):\n\u001b[1;32m    192\u001b[0m     \u001b[39mfor\u001b[39;00m layer \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers:\n\u001b[1;32m    193\u001b[0m         \u001b[39m# Changed order\u001b[39;00m\n\u001b[0;32m--> 194\u001b[0m         x \u001b[39m=\u001b[39m layer(x, enc_out, enc_out, self_mask\u001b[39m=\u001b[39;49mself_mask, enc_mask\u001b[39m=\u001b[39;49menc_mask)\n\u001b[1;32m    195\u001b[0m     \u001b[39mreturn\u001b[39;00m x\n",
            "File \u001b[0;32m~/.conda/envs/NLP/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[0;32m~/Projects/R/Assigments/Project/transfomer.py:120\u001b[0m, in \u001b[0;36mDecoderBlock.forward\u001b[0;34m(self, x, keys, values, self_mask, enc_mask)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x, keys, values, self_mask, enc_mask):\n\u001b[1;32m    118\u001b[0m \n\u001b[1;32m    119\u001b[0m     \u001b[39m# Self attention\u001b[39;00m\n\u001b[0;32m--> 120\u001b[0m     self_attention \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mself_attention(x, x, x, self_mask)\n\u001b[1;32m    121\u001b[0m     \u001b[39m# Residual\u001b[39;00m\n\u001b[1;32m    122\u001b[0m     self_attention \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlnorm1(self_attention \u001b[39m+\u001b[39m x))\n",
            "File \u001b[0;32m~/.conda/envs/NLP/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[0;32m~/Projects/R/Assigments/Project/transfomer.py:36\u001b[0m, in \u001b[0;36mMultiHeadAttention.forward\u001b[0;34m(self, Q, K, V, mask)\u001b[0m\n\u001b[1;32m     33\u001b[0m V \u001b[39m=\u001b[39m V\u001b[39m.\u001b[39mreshape(N, k_len, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mheads, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mheads_dim)\n\u001b[1;32m     35\u001b[0m \u001b[39m# 2) Compute Attention\u001b[39;00m\n\u001b[0;32m---> 36\u001b[0m attention \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49meinsum(\u001b[39m\"\u001b[39;49m\u001b[39mnqhd,nkhd->nhqk\u001b[39;49m\u001b[39m\"\u001b[39;49m, [Q, K])\n\u001b[1;32m     37\u001b[0m \u001b[39m# attention = (N, heads, q_len, k_len)\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[39mif\u001b[39;00m mask \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
            "File \u001b[0;32m~/.conda/envs/NLP/lib/python3.10/site-packages/torch/functional.py:358\u001b[0m, in \u001b[0;36meinsum\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    355\u001b[0m     _operands \u001b[39m=\u001b[39m operands[\u001b[39m0\u001b[39m]\n\u001b[1;32m    356\u001b[0m     \u001b[39m# recurse incase operands contains value that has torch function\u001b[39;00m\n\u001b[1;32m    357\u001b[0m     \u001b[39m# in the original implementation this line is omitted\u001b[39;00m\n\u001b[0;32m--> 358\u001b[0m     \u001b[39mreturn\u001b[39;00m einsum(equation, \u001b[39m*\u001b[39;49m_operands)\n\u001b[1;32m    360\u001b[0m \u001b[39mreturn\u001b[39;00m _VF\u001b[39m.\u001b[39meinsum(equation, operands)\n",
            "File \u001b[0;32m~/.conda/envs/NLP/lib/python3.10/site-packages/torch/functional.py:360\u001b[0m, in \u001b[0;36meinsum\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    356\u001b[0m     \u001b[39m# recurse incase operands contains value that has torch function\u001b[39;00m\n\u001b[1;32m    357\u001b[0m     \u001b[39m# in the original implementation this line is omitted\u001b[39;00m\n\u001b[1;32m    358\u001b[0m     \u001b[39mreturn\u001b[39;00m einsum(equation, \u001b[39m*\u001b[39m_operands)\n\u001b[0;32m--> 360\u001b[0m \u001b[39mreturn\u001b[39;00m _VF\u001b[39m.\u001b[39;49meinsum(equation, operands)\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "evaluate(model, dataloader_test, writer, de_token)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.10.4 ('NLP')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "47d1cf54bf9cf5fce4b000eacb105df7d7e5f1fe165267018e0a6855939e5736"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

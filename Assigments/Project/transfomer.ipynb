{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YcP7qZGOA_4f"
      },
      "outputs": [],
      "source": [
        "!pip install torchmetrics datasets tokenizers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LI4lW4x9A_4i"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "wmt14 = load_dataset('wmt14', 'de-en')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DTHjttReBG5f"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/hynky1999/Statistical-learning-class\n",
        "%cd /content/Statistical-learning-class/Assigments/Project"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aD2yRsMyA_4j"
      },
      "outputs": [],
      "source": [
        "train_subset_length = 100000\n",
        "test_subset_length = 2000\n",
        "vocab_size=40000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yp9xKHWeA_4j"
      },
      "outputs": [],
      "source": [
        "train_dataset = wmt14['train'].select(range(train_subset_length))\n",
        "test_dataset = wmt14['test'].select(range(test_subset_length))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jPGN6kzyA_4k"
      },
      "outputs": [],
      "source": [
        "from tokenizers.models import BPE\n",
        "from tokenizers import Tokenizer\n",
        "from tokenizers.trainers import BpeTrainer\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "from tokenizers.processors import TemplateProcessing\n",
        "\n",
        "def create_tokenizer(iterable, add_special_tokens=False):\n",
        "    trainer = BpeTrainer(vocab_size=vocab_size, show_progress=True, special_tokens=[\"[PAD]\",\"[UNK]\"])\n",
        "    tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
        "    tokenizer.train_from_iterator(iterable, trainer=trainer)\n",
        "    if add_special_tokens:\n",
        "        tokenizer.add_special_tokens([\"[START]\", \"[END]\"])\n",
        "        START_ID, END_ID = tokenizer.token_to_id(\"[START]\"), tokenizer.token_to_id(\"[END]\")\n",
        "        tokenizer.post_processor = TemplateProcessing(single=\"[START] $A [END]\", special_tokens=[(\"[START]\", START_ID), (\"[END]\", END_ID)])\n",
        "\n",
        "\n",
        "    tokenizer.enable_padding(pad_token=\"[PAD]\", pad_id=tokenizer.token_to_id(\"[PAD]\"))\n",
        "    return tokenizer\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GGPb-9rVA_4k"
      },
      "outputs": [],
      "source": [
        "\n",
        "de_it = map(lambda x: x['de'] , train_dataset['translation'])\n",
        "en_it = map(lambda x: x['en'] , train_dataset['translation'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NqR6TTiXA_4k"
      },
      "outputs": [],
      "source": [
        "de_token = create_tokenizer(de_it, add_special_tokens=True)\n",
        "en_token = create_tokenizer(en_it)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P8VMntg2A_4l"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy.random as rnd\n",
        "from torch.utils.data.dataloader import Sampler\n",
        "class BatchedSampler(Sampler[int]):\n",
        "\n",
        "  def __init__(self, data_source, batch_size) -> None:\n",
        "      self.data_source = data_source\n",
        "      self.batch_size = batch_size\n",
        "\n",
        "  def __iter__(self):\n",
        "      batches = len(self.data_source)//self.batch_size\n",
        "      perm = rnd.permutation(list(range(batches)))\n",
        "      permutated_batches = [range(p*self.batch_size, (p+1)*self.batch_size) for p in perm]\n",
        "      permutated_ind = [ind for b in permutated_batches for ind in b]\n",
        "      permutated_ind.extend(range((batches)*self.batch_size, min((batches + 1)*self.batch_size, len(self.data_source))))\n",
        "      return iter(permutated_ind)\n",
        "\n",
        "  def __len__(self):\n",
        "      return len(self.data_source)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mqtTUtjKA_4l"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "def extract_embedding(embeds, lang):\n",
        "    return {f\"{lang}_ids\": [e.ids for e in embeds], f\"{lang}_att\": [e.attention_mask for e in embeds]}\n",
        "\n",
        "def add_lenghts(trans):\n",
        "  translation = trans[\"translation\"]\n",
        "  de_sent = [t[\"de\"] for t in translation]\n",
        "  en_sent = [t[\"en\"] for t in translation]\n",
        "  en_len = [len(t[\"en\"]) for t in translation]\n",
        "  de_len = [len(t[\"de\"]) for t in translation]\n",
        "  return {\"en_len\": en_len, \"de_len\": de_len,\"de_sent\": de_sent, \"en_sent\": en_sent}\n",
        "\n",
        "\n",
        "\n",
        "def tokenize(trans):\n",
        "    translation = trans[\"translation\"]\n",
        "    de_sent = [t[\"de\"] for t in translation]\n",
        "    en_sent = [t[\"en\"] for t in translation]\n",
        "    en = en_token.encode_batch(en_sent)\n",
        "    de = de_token.encode_batch(de_sent)\n",
        "    dct = {**extract_embedding(en, \"en\"), **extract_embedding(de, \"de\")}\n",
        "    return dct\n",
        "\n",
        "def collate_fc(batch):\n",
        "    en_ids = torch.stack([b[\"en_ids\"] for b in batch])\n",
        "    en_att = torch.stack([b[\"en_att\"] for b in batch]).unsqueeze(1).unsqueeze(1)\n",
        "    de_ids = torch.stack([b[\"de_ids\"] for b in batch])\n",
        "    de_att = torch.stack([b[\"de_att\"] for b in batch]).unsqueeze(1).unsqueeze(1)\n",
        "    de_sent = [b[\"de_sent\"] for b in batch]\n",
        "    en_sent = [b[\"en_sent\"] for b in batch]\n",
        "\n",
        "    return {\"en_ids\": en_ids, \"de_ids\": de_ids, \"en_att\": en_att, \"de_att\": de_att, \"de_sent\": de_sent, \"en_sent\": en_sent}\n",
        "\n",
        "def create_dataloader(dataset, batch_size=32, shuffle=False):\n",
        "    tokenized = dataset.map(add_lenghts, batch_size=batch_size, batched=True)\n",
        "    # Sort by lengths to get smaller paddings\n",
        "    tokenized = tokenized.sort(\"en_len\")\n",
        "    tokenized = tokenized.sort(\"de_len\",kind=\"stable\")\n",
        "    tokenized = tokenized.map(tokenize, batch_size=batch_size, batched=True)\n",
        "    tokenized = tokenized.remove_columns(\"translation\")\n",
        "    tokenized.set_format(\"torch\")\n",
        "    return DataLoader(tokenized, batch_size=batch_size, shuffle=False,collate_fn=collate_fc, sampler=BatchedSampler(tokenized, batch_size))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Kz4NTnXA_4m"
      },
      "outputs": [],
      "source": [
        "dataloader_train = create_dataloader(train_dataset, batch_size=32, shuffle=False)\n",
        "dataloader_test = create_dataloader(test_dataset, batch_size=32, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8HPiuLLVA_4m"
      },
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-ecmFfF2A_4n"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oxwr40aGA_4n"
      },
      "outputs": [],
      "source": [
        "from train_test import train, evaluate\n",
        "import torch.nn as nn\n",
        "from torch.utils.tensorboard import SummaryWriter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DtocloMZA_4n"
      },
      "outputs": [],
      "source": [
        "from model import WMTModel\n",
        "d_model=128\n",
        "model = WMTModel(en_token.get_vocab_size(), de_token.get_vocab_size() , d_model)\n",
        "# Set to square root of model\n",
        "# Then multiply by  min(step_num^−0.5 , step_num * warmup_steps^−1.5)\n",
        "initial_lr = d_model ** -0.5\n",
        "warmup_steps = 4000\n",
        "multiplier_lambda = lambda step: min((step+1) ** -0.5, (step+1) * warmup_steps ** -1.5)\n",
        "optimizer = torch.optim.Adam(model.parameters(), betas=(0.9, 0.98), eps=1e-9, lr=initial_lr)\n",
        "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, multiplier_lambda)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
        "writer = SummaryWriter()\n",
        "epochs=10\n",
        "for epoch in range(epochs):\n",
        "    train(model, optimizer, scheduler ,criterion, dataloader_train, writer, epoch, minibatch=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VjOgcFxmA_4u"
      },
      "outputs": [],
      "source": [
        "evaluate(model, dataloader_test, writer, de_token)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.10.4 ('NLP')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4 (main, Mar 31 2022, 08:41:55) [GCC 7.5.0]"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "47d1cf54bf9cf5fce4b000eacb105df7d7e5f1fe165267018e0a6855939e5736"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

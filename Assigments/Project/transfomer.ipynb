{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "YcP7qZGOA_4f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torchmetrics in /home/kydliceh/.conda/envs/NLP/lib/python3.10/site-packages (0.10.1)\n",
            "Requirement already satisfied: datasets in /home/kydliceh/.conda/envs/NLP/lib/python3.10/site-packages (2.8.0)\n",
            "Requirement already satisfied: tokenizers in /home/kydliceh/.conda/envs/NLP/lib/python3.10/site-packages (0.11.4)\n",
            "Requirement already satisfied: torch>=1.3.1 in /home/kydliceh/.conda/envs/NLP/lib/python3.10/site-packages (from torchmetrics) (1.12.1)\n",
            "Requirement already satisfied: packaging in /home/kydliceh/.conda/envs/NLP/lib/python3.10/site-packages (from torchmetrics) (21.3)\n",
            "Requirement already satisfied: numpy>=1.17.2 in /home/kydliceh/.local/lib/python3.10/site-packages (from torchmetrics) (1.23.3)\n",
            "Requirement already satisfied: pyarrow>=6.0.0 in /home/kydliceh/.conda/envs/NLP/lib/python3.10/site-packages (from datasets) (8.0.0)\n",
            "Requirement already satisfied: dill<0.3.7 in /home/kydliceh/.conda/envs/NLP/lib/python3.10/site-packages (from datasets) (0.3.6)\n",
            "Requirement already satisfied: pandas in /home/kydliceh/.local/lib/python3.10/site-packages (from datasets) (1.4.4)\n",
            "Requirement already satisfied: requests>=2.19.0 in /home/kydliceh/.conda/envs/NLP/lib/python3.10/site-packages (from datasets) (2.28.1)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /home/kydliceh/.conda/envs/NLP/lib/python3.10/site-packages (from datasets) (4.64.0)\n",
            "Requirement already satisfied: xxhash in /home/kydliceh/.conda/envs/NLP/lib/python3.10/site-packages (from datasets) (0.0.0)\n",
            "Requirement already satisfied: multiprocess in /home/kydliceh/.conda/envs/NLP/lib/python3.10/site-packages (from datasets) (0.70.12.2)\n",
            "Requirement already satisfied: fsspec[http]>=2021.11.1 in /home/kydliceh/.conda/envs/NLP/lib/python3.10/site-packages (from datasets) (2022.11.0)\n",
            "Requirement already satisfied: aiohttp in /home/kydliceh/.conda/envs/NLP/lib/python3.10/site-packages (from datasets) (3.8.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.2.0 in /home/kydliceh/.conda/envs/NLP/lib/python3.10/site-packages (from datasets) (0.11.1)\n",
            "Requirement already satisfied: responses<0.19 in /home/kydliceh/.conda/envs/NLP/lib/python3.10/site-packages (from datasets) (0.18.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /home/kydliceh/.conda/envs/NLP/lib/python3.10/site-packages (from datasets) (6.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /home/kydliceh/.conda/envs/NLP/lib/python3.10/site-packages (from aiohttp->datasets) (21.4.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /home/kydliceh/.conda/envs/NLP/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.2)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /home/kydliceh/.local/lib/python3.10/site-packages (from aiohttp->datasets) (2.0.12)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /home/kydliceh/.local/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /home/kydliceh/.local/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.2)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /home/kydliceh/.conda/envs/NLP/lib/python3.10/site-packages (from aiohttp->datasets) (1.2.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /home/kydliceh/.conda/envs/NLP/lib/python3.10/site-packages (from aiohttp->datasets) (1.8.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/kydliceh/.local/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (4.4.0)\n",
            "Requirement already satisfied: filelock in /home/kydliceh/.conda/envs/NLP/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (3.8.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/kydliceh/.conda/envs/NLP/lib/python3.10/site-packages (from packaging->torchmetrics) (3.0.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /home/kydliceh/.conda/envs/NLP/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/kydliceh/.conda/envs/NLP/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (1.26.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /home/kydliceh/.conda/envs/NLP/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (3.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /home/kydliceh/.local/lib/python3.10/site-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /home/kydliceh/.local/lib/python3.10/site-packages (from pandas->datasets) (2022.2.1)\n",
            "Requirement already satisfied: six>=1.5 in /home/kydliceh/.conda/envs/NLP/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install torchmetrics datasets tokenizers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "LI4lW4x9A_4i"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Found cached dataset wmt14 (/home/kydliceh/.cache/huggingface/datasets/wmt14/de-en/1.0.0/2de185b074515e97618524d69f5e27ee7545dcbed4aa9bc1a4235710ffca33f4)\n"
          ]
        },
        {
          "data": {
            "application/json": {
              "ascii": false,
              "bar_format": null,
              "colour": null,
              "elapsed": 0.034238576889038086,
              "initial": 0,
              "n": 0,
              "ncols": null,
              "nrows": null,
              "postfix": null,
              "prefix": "",
              "rate": null,
              "total": 3,
              "unit": "it",
              "unit_divisor": 1000,
              "unit_scale": false
            },
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2b815b6e124743c59000088c68b13318",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "wmt14 = load_dataset('wmt14', 'de-en')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "DTHjttReBG5f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'Statistical-learning-class'...\n",
            "remote: Enumerating objects: 206, done.\u001b[K\n",
            "remote: Counting objects: 100% (206/206), done.\u001b[K\n",
            "remote: Compressing objects: 100% (142/142), done.\u001b[K\n",
            "remote: Total 206 (delta 54), reused 177 (delta 29), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (206/206), 1.58 MiB | 5.23 MiB/s, done.\n",
            "Resolving deltas: 100% (54/54), done.\n",
            "[Errno 2] No such file or directory: '/content/Statistical-learning-class/Assigments/Project'\n",
            "/home/kydliceh/Projects/R/Assigments/Project\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/hynky1999/Statistical-learning-class\n",
        "%cd /content/Statistical-learning-class/Assigments/Project"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "aD2yRsMyA_4j"
      },
      "outputs": [],
      "source": [
        "train_subset_length = 10\n",
        "test_subset_length = 2000\n",
        "vocab_size=10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "yp9xKHWeA_4j"
      },
      "outputs": [],
      "source": [
        "train_dataset = wmt14['train'].select(range(train_subset_length))\n",
        "test_dataset = wmt14['test'].select(range(test_subset_length))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {},
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "type 'tokenizers.processors.PostProcessor' is not an acceptable base type",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn [64], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mProcessorSequence\u001b[39;00m(processors\u001b[39m.\u001b[39mPostProcessor):\n\u001b[1;32m      2\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, processors):\n\u001b[1;32m      3\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprocessors \u001b[39m=\u001b[39m processors\n",
            "\u001b[0;31mTypeError\u001b[0m: type 'tokenizers.processors.PostProcessor' is not an acceptable base type"
          ]
        }
      ],
      "source": [
        "class ProcessorSequence(processors.PostProcessor):\n",
        "    def __init__(self, processors):\n",
        "        self.processors = processors\n",
        "    \n",
        "    def num_special_tokens_to_add(self, pair=False):\n",
        "        return sum(p.num_special_tokens_to_add(pair) for p in self.processors)\n",
        "\n",
        "    def process(self, encoding, pair=None, add_special_tokens=True):\n",
        "        for p in self.processors:\n",
        "            encoding = p.process(encoding, pair, add_special_tokens)\n",
        "        return encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "id": "jPGN6kzyA_4k"
      },
      "outputs": [],
      "source": [
        "from tokenizers import ByteLevelBPETokenizer\n",
        "\n",
        "\n",
        "\n",
        "def create_tokenizer(iterable, add_special_tokens=False, train=True, vocab_size=1000):\n",
        "    toknizer = ByteLevelBPETokenizer()\n",
        "    if train:\n",
        "        toknizer.train_from_iterator(iterable, vocab_size=vocab_size, min_frequency=2, special_tokens=[\"[PAD]\"])\n",
        "\n",
        "    if add_special_tokens:\n",
        "        toknizer.add_special_tokens([\"[START]\", \"[END]\"])\n",
        "    return toknizer\n",
        "\n",
        "\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "id": "GGPb-9rVA_4k"
      },
      "outputs": [],
      "source": [
        "\n",
        "de_it = map(lambda x: x['de'] , train_dataset['translation'])\n",
        "en_it = map(lambda x: x['en'] , train_dataset['translation'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'de_token' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn [49], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m de_token \u001b[39m=\u001b[39m de_token\u001b[39m.\u001b[39mfrom_file(\u001b[39m\"\u001b[39m\u001b[39mde_token.json\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      2\u001b[0m en_token \u001b[39m=\u001b[39m en_token\u001b[39m.\u001b[39mfrom_file(\u001b[39m\"\u001b[39m\u001b[39men_token.json\u001b[39m\u001b[39m\"\u001b[39m)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'de_token' is not defined"
          ]
        }
      ],
      "source": [
        "de_token = de_token.from_file(\"de_token.json\")\n",
        "en_token = en_token.from_file(\"en_token.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "id": "NqR6TTiXA_4k"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "de_token = create_tokenizer(de_it, train=True, vocab_size=vocab_size, add_special_tokens=True)\n",
        "en_token = create_tokenizer(en_it, train=True, vocab_size=vocab_size)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [],
      "source": [
        "de_token.save(\"de_token.json\")\n",
        "en_token.save(\"en_token.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "id": "P8VMntg2A_4l"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 122,
      "metadata": {
        "id": "mqtTUtjKA_4l"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "def extract_embedding(embeds, lang):\n",
        "    return {f\"{lang}_ids\": [e.ids for e in embeds], f\"{lang}_att\": [e.attention_mask for e in embeds]}\n",
        "\n",
        "def add_lenghts(trans):\n",
        "  translation = trans[\"translation\"]\n",
        "  de_sent = [t[\"de\"] for t in translation]\n",
        "  en_sent = [t[\"en\"] for t in translation]\n",
        "  en_len = [len(t[\"en\"]) for t in translation]\n",
        "  de_len = [len(t[\"de\"]) for t in translation]\n",
        "  return {\"en_len\": en_len, \"de_len\": de_len,\"de_sent\": de_sent, \"en_sent\": en_sent}\n",
        "\n",
        "\n",
        "\n",
        "def tokenize(trans):\n",
        "    translation = trans[\"translation\"]\n",
        "    de_sent = [t[\"de\"] for t in translation]\n",
        "    en_sent = [t[\"en\"] for t in translation]\n",
        "    en = en_token.encode_batch(en_sent)\n",
        "    de = de_token.encode_batch(de_sent)\n",
        "    dct = {**extract_embedding(en, \"en\"), **extract_embedding(de, \"de\")}\n",
        "    return dct\n",
        "\n",
        "def collate_fc(batch):\n",
        "    en_ids_padded = torch.nn.utils.rnn.pad_sequence([b[\"en_ids\"] for b in batch], batch_first=True, padding_value=en_token.token_to_id(\"[PAD]\"))\n",
        "\n",
        "    de_ids = [torch.cat([torch.tensor([de_token.token_to_id(\"[START]\")]), b[\"de_ids\"], torch.tensor([de_token.token_to_id(\"[END]\")])]) for b in batch]\n",
        "\n",
        "    de_ids_padded = torch.nn.utils.rnn.pad_sequence(de_ids, batch_first=True, padding_value=de_token.token_to_id(\"[PAD]\"))\n",
        "\n",
        "    en_sents = [b[\"en_sent\"] for b in batch]\n",
        "    de_sents = [b[\"de_sent\"] for b in batch]\n",
        "\n",
        "    return {\"en_ids\": en_ids_padded, \"de_ids\": de_ids_padded, \"de_sent\": de_sents, \"en_sent\": en_sents}\n",
        "\n",
        "def create_dataloader(dataset, batch_size=32, shuffle=False):\n",
        "    tokenized = dataset.map(add_lenghts, batch_size=batch_size, batched=True)\n",
        "    # Sort by lengths to get smaller paddings\n",
        "    tokenized = tokenized.sort(\"en_len\")\n",
        "    tokenized = tokenized.sort(\"de_len\",kind=\"stable\")\n",
        "    tokenized = tokenized.map(tokenize, batch_size=batch_size, batched=True)\n",
        "    tokenized = tokenized.remove_columns(\"translation\")\n",
        "    tokenized.set_format(\"torch\")\n",
        "    return DataLoader(tokenized, batch_size=batch_size, shuffle=True,collate_fn=collate_fc)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 123,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "258"
            ]
          },
          "execution_count": 123,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "de_token.token_to_id(\"[END]\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 126,
      "metadata": {
        "id": "5Kz4NTnXA_4m"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading cached processed dataset at /home/kydliceh/.cache/huggingface/datasets/wmt14/de-en/1.0.0/2de185b074515e97618524d69f5e27ee7545dcbed4aa9bc1a4235710ffca33f4/cache-865161eab3354bb8.arrow\n",
            "Loading cached sorted indices for dataset at /home/kydliceh/.cache/huggingface/datasets/wmt14/de-en/1.0.0/2de185b074515e97618524d69f5e27ee7545dcbed4aa9bc1a4235710ffca33f4/cache-65228b20db78c273.arrow\n",
            "Loading cached sorted indices for dataset at /home/kydliceh/.cache/huggingface/datasets/wmt14/de-en/1.0.0/2de185b074515e97618524d69f5e27ee7545dcbed4aa9bc1a4235710ffca33f4/cache-61ec2f49f1cec7c6.arrow\n",
            "Loading cached processed dataset at /home/kydliceh/.cache/huggingface/datasets/wmt14/de-en/1.0.0/2de185b074515e97618524d69f5e27ee7545dcbed4aa9bc1a4235710ffca33f4/cache-51df25cc70a98ecc.arrow\n",
            "Loading cached processed dataset at /home/kydliceh/.cache/huggingface/datasets/wmt14/de-en/1.0.0/2de185b074515e97618524d69f5e27ee7545dcbed4aa9bc1a4235710ffca33f4/cache-593bbaf8b7994ff6.arrow\n",
            "Loading cached sorted indices for dataset at /home/kydliceh/.cache/huggingface/datasets/wmt14/de-en/1.0.0/2de185b074515e97618524d69f5e27ee7545dcbed4aa9bc1a4235710ffca33f4/cache-f653097da3aa98d2.arrow\n",
            "Loading cached sorted indices for dataset at /home/kydliceh/.cache/huggingface/datasets/wmt14/de-en/1.0.0/2de185b074515e97618524d69f5e27ee7545dcbed4aa9bc1a4235710ffca33f4/cache-9a656fc64426643c.arrow\n",
            "Loading cached processed dataset at /home/kydliceh/.cache/huggingface/datasets/wmt14/de-en/1.0.0/2de185b074515e97618524d69f5e27ee7545dcbed4aa9bc1a4235710ffca33f4/cache-6fec467a96d37603.arrow\n"
          ]
        }
      ],
      "source": [
        "dataloader_train = create_dataloader(train_dataset, batch_size=32, shuffle=True)\n",
        "dataloader_test = create_dataloader(test_dataset, batch_size=32, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "8HPiuLLVA_4m"
      },
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "-ecmFfF2A_4n"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "Oxwr40aGA_4n"
      },
      "outputs": [],
      "source": [
        "from train_test import train, evaluate\n",
        "import torch.nn as nn\n",
        "from torch.utils.tensorboard import SummaryWriter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "DtocloMZA_4n"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-01-02 09:20:53.743446: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-01-02 09:20:54.830852: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
            "2023-01-02 09:20:54.830879: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
            "2023-01-02 09:20:58.077854: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
            "2023-01-02 09:20:58.079898: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
            "2023-01-02 09:20:58.080059: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
          ]
        },
        {
          "ename": "ZeroDivisionError",
          "evalue": "integer division or modulo by zero",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn [17], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m save_every\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m\n\u001b[1;32m     15\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(epochs):\n\u001b[0;32m---> 16\u001b[0m     train(model, optimizer, scheduler ,criterion, dataloader_train, writer, epoch, minibatch\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m     18\u001b[0m     \u001b[39mif\u001b[39;00m epoch \u001b[39m%\u001b[39m save_every \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m     19\u001b[0m         torch\u001b[39m.\u001b[39msave(model\u001b[39m.\u001b[39mstate_dict(), \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mmodel_\u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m}\u001b[39;00m\u001b[39m.pt\u001b[39m\u001b[39m\"\u001b[39m)\n",
            "File \u001b[0;32m~/Projects/R/Assigments/Project/train_test.py:67\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, optimizer, scheduler, criterion, train_data, writer, epoch, minibatch, device)\u001b[0m\n\u001b[1;32m     64\u001b[0m scheduler\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     66\u001b[0m total_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()\n\u001b[0;32m---> 67\u001b[0m \u001b[39mif\u001b[39;00m batch_i \u001b[39m%\u001b[39;49m interval \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m batch_i \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m \u001b[39mor\u001b[39;00m minibatch:\n\u001b[1;32m     68\u001b[0m     create_report(\n\u001b[1;32m     69\u001b[0m         writer, total_loss \u001b[39m/\u001b[39m interval, batch_i, total_batches, start_time, epoch\n\u001b[1;32m     70\u001b[0m     )\n\u001b[1;32m     71\u001b[0m     total_loss \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n",
            "\u001b[0;31mZeroDivisionError\u001b[0m: integer division or modulo by zero"
          ]
        }
      ],
      "source": [
        "from model import WMTModel\n",
        "d_model=128\n",
        "model = WMTModel(en_token.get_vocab_size(), de_token.get_vocab_size() , d_model)\n",
        "# Set to square root of model\n",
        "# Then multiply by  min(step_num^−0.5 , step_num * warmup_steps^−1.5)\n",
        "initial_lr = d_model ** -0.5\n",
        "warmup_steps = 4000\n",
        "multiplier_lambda = lambda step: min((step+1) ** -0.5, (step+1) * warmup_steps ** -1.5)\n",
        "optimizer = torch.optim.Adam(model.parameters(), betas=(0.9, 0.98), eps=1e-9, lr=initial_lr)\n",
        "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, multiplier_lambda)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
        "writer = SummaryWriter()\n",
        "epochs=10\n",
        "save_every=2\n",
        "for epoch in range(epochs):\n",
        "    train(model, optimizer, scheduler ,criterion, dataloader_train, writer, epoch, minibatch=False)\n",
        "\n",
        "    if epoch % save_every == 0:\n",
        "        torch.save(model.state_dict(), f\"model_{epoch}.pt\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VjOgcFxmA_4u"
      },
      "outputs": [],
      "source": [
        "evaluate(model, dataloader_test, writer, de_token)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.10.4 ('NLP')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "47d1cf54bf9cf5fce4b000eacb105df7d7e5f1fe165267018e0a6855939e5736"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "09ehUJkD03Qw"
   },
   "source": [
    "# Assignment 2, task 3\n",
    "**Xijia Liu, Umeå University**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sJQEGE_AKW8D"
   },
   "source": [
    "## Task 3: The soul of deep learning, gradient\n",
    "\n",
    "**Background**: deep learning has obtained great success, however, it still has many issues that are unclear. Proper optimization methods to solve the non-convex problem in deep learning is still an open question. In fact, we are empirically using some brute force methods to find the optimal parameter estimation in almost all deep learning applications. In those brute force methods, gradient and its calculation play a key role and therefore it is the soul of deep learning.\n",
    "\n",
    "**Task description**: in this task, we investigate the inner working mechanism of Tensorflow. A BIG remark first. Different from the previous task, We will try to get rid of the bondage of Keras, bypass the \"model compile -> model training\" routine, and use the gradient calculation function of Tensorflow to implement the gradient descent algorithm through our own code.\n",
    "\n",
    "There will be 3 subtasks. First, we learn how to calculate the gradient in Tensorflow. Second, implement the Gradient descent algorithm to a simple numerical problem. Third, apply the Gradient descent algorithm to learn a simple linear regression model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J6eOsNSOovMe"
   },
   "source": [
    "###Task 3.1 Understand gradient and its calculation in TF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jvArLR4rPBE7"
   },
   "source": [
    "I am from Umeå and skiing is one of our daily basic survival skills. I learned downhill ski through the theory of gradient. So, let's understand gradient by the following gif pitcure.\n",
    "\n",
    "![picture](https://j.gifs.com/y72Q7Y.gif)\n",
    "\n",
    "1. Gradient is a generalization of derivative for a scalar-valued multivariable function.\n",
    "2. Consider $z = f(x, y)$, gradient (vector) of function $z$ is a 2-dim vector of partial derivatives of each varaible. \n",
    "3. $ \\nabla f = (\\frac{\\partial f}{\\partial x}, \\frac{\\partial f}{\\partial y})^{\\top}$\n",
    "4. The opposite direction of a gradient vector points to the direction of steepest descent. \n",
    "\n",
    "Next, we learn how to evaluate the gradient vector of a multivariable function at certain point. Suppose we want to use tensorflow to calculate the derivative of function \n",
    "$$\n",
    "  f(x) = 2x^2 + 3x + 1\n",
    "$$ \n",
    "at $x_0 = 3$. The derivative can be easily calculated as $f'(x_0) = 4x_0+3 = 15$. How does TF handle this trivial example? Let's see the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 5345,
     "status": "ok",
     "timestamp": 1669457797285,
     "user": {
      "displayName": "Xijia Liu",
      "userId": "07624774260162876472"
     },
     "user_tz": -60
    },
    "id": "Nbxw57s-QgPL"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-06 17:53:25.944300: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-12-06 17:53:26.402857: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-12-06 17:53:26.402893: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-12-06 17:53:28.872783: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-12-06 17:53:28.873997: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-12-06 17:53:28.874114: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf #import TF moudle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "0gW-KdlRP4bm"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the derivative is  15.0\n"
     ]
    }
   ],
   "source": [
    "x_0 = 3.\n",
    "x = tf.Variable(x_0)\n",
    "with tf.GradientTape() as tape:\n",
    "  f = 2*x**2+3*x+1\n",
    "  grad = tape.gradient(f, x)\n",
    "  print('the derivative is ', float(grad))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VQFV6ydC2NNy"
   },
   "source": [
    "The first important thing for evaluating gradient in TensforFlow is define a TensorFlow variable, for example, line 2. Then we can apply function 'GradientTape()' to evaluate the gradient for a function. Here, we can sue Python 'with-as' syntax to define the function and evaluate its gradient efficiently.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EWqw5YLRPI1i"
   },
   "source": [
    "**Now, it is your turn.** Can you write a few of lines code to calculate the gradient vector of a function of three variables? \n",
    "$$f(x,y,z) = x^3 + 3y^3 + 2z^3 + 2x^2y + 3xyz $$The gradient is $$ \\nabla f = \\begin{pmatrix}\n",
    "3x^2+4xy+3yz\\\\ \n",
    "9y^2+2x^2+3xz\\\\ \n",
    "6z^2+3xy\n",
    "\\end{pmatrix} $$\n",
    "Then your task is apply TF's functions to evaluate the gradient at $(x,y,z)^{T} = (2,4,1)^{T}$.\n",
    "\n",
    "**Tips**: you need to define three tensorflow variables and put them in a array '[x,y,z]' when you apply function 'tape.gradient()' to evaluate the gradient. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "gU-pE2SNe9Ee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the derivatives are  [2072.0, 859.78271484375, 28.0]\n"
     ]
    }
   ],
   "source": [
    "# solutions\n",
    "x_0 = 2.\n",
    "y_0 = 4.\n",
    "z_0 = 1.\n",
    "x = tf.Variable(x_0)\n",
    "y = tf.Variable(y_0)\n",
    "z = tf.Variable(z_0)\n",
    "with tf.GradientTape() as tape:\n",
    "  f = x**3 + 3*y**3 + 2*z**2 + 2*x**(2*y) + 3*x*y*z\n",
    "  grad = tape.gradient(f, [x, y, z])\n",
    "  print('the derivatives are ', [float(grad[0]), float(grad[1]), float(grad[2])])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TvceSjY9VVi5"
   },
   "source": [
    "### Task 3.2 Gradient descent algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xl7_hgWNVdRJ"
   },
   "source": [
    "Since the negative gradient vector points to the direction of the steepest \n",
    "Suppose we want to minimize an objective function $f(\\textbf{w})$. Since the negative gradient vector, $\\nabla f(\\textbf{w})$, points to the direction of the steepest descent of the objective function, it provides us an idea for finding the minimum value of an objective function. We start from an initial value of the optimizing variable and evaluate the gradient vector at this point, then update the optimizing variable toward the negative gradient direction with a small step, i.e. \n",
    "$$\n",
    "\\textbf{w}_{t+1} = \\textbf{w}_{t} - \\alpha \\nabla f(\\textbf{w}_t)\n",
    "$$ \n",
    "Repeat this procedure many times, then we can find the minimiaer. A one dimensional and a two dimensional example are displayed below.\n",
    "\n",
    "![picture](https://hackernoon.com/hn-images/1*ZmzSnV6xluGa42wtU7KYVA.gif)\n",
    "![picture](https://upload.wikimedia.org/wikipedia/commons/a/a3/Gradient_descent.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NpL3ukYTWLNk"
   },
   "source": [
    "**Example**: suppose we want to find the minimum value of function $f(x) = x^2-5x-1$ and the minimizer. We can implement the Gradient descent alogrithm as the following code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1669465064957,
     "user": {
      "displayName": "Xijia Liu",
      "userId": "07624774260162876472"
     },
     "user_tz": -60
    },
    "id": "tPQIRBvSZoFT",
    "outputId": "ac2f0d93-c995-4d63-d79a-6ae4c3651346"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4969050884246826\n"
     ]
    }
   ],
   "source": [
    "lr = 0.1\n",
    "x = tf.Variable(0, name=\"variable\", dtype=tf.float32)\n",
    "for i in range (30):\n",
    "  with tf.GradientTape() as tape:\n",
    "    f = x**2-5*x-1\n",
    "    grads = tape.gradient(f, x)\n",
    "    x.assign_sub(lr*grads)\n",
    "print(float(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VIjUbxpEfO8S"
   },
   "source": [
    "**Remarks**: \n",
    "1. To verify the results, we can find the minimizer by calculating the partial derivative and find the stationary point\n",
    "$$\n",
    "  f'(x) = 2x-5 = 0\n",
    "$$\n",
    "The minimizer is $2.5$\n",
    "2. In the code, line 6, it is the way to update a variable in TensorFlow. Method '.assign_sub' means update 'x' by substract it by 'lr*grads'. You may guess how to update 'x' if we set 'lr = -0.1'\n",
    "3. You also can play this code with different learning rate and summarize the effects of learning rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now, it is your turn.** In this subtask, you need to write a program to find the minimizer of a bivariable objective function \n",
    "  $$\n",
    "    f(x,y) = (x-1)^2+0.5(y-3)^2\n",
    "  $$\n",
    "**Tips**: \n",
    "1. You can use a while loop with an approximation threshold value, or just simply use a for loop, to find minimizer.\n",
    "2. The initial value can be set as $(x,y)^{\\top} = (0,0)^{\\top}$ and you can choose a proper learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last value: 0.000498245470225811 Current value: 0.0004972525057382882\n",
      "Minimizer: (0.9998897910118103, 2.9684646129608154), Minimum: 0.0004972525057382882 Iterations: 4554 with lr: 0.001\n",
      "Last value: 4.952193921781145e-05 Current value: 4.8537436668993905e-05\n",
      "Minimizer: (0.9999898672103882, 2.990147352218628), Minimum: 4.8537436668993905e-05 Iterations: 570 with lr: 0.01\n",
      "Last value: 5.06690639667795e-06 Current value: 4.104330400878098e-06\n",
      "Minimizer: (0.9999995827674866, 2.9971349239349365), Minimum: 4.104330400878098e-06 Iterations: 67 with lr: 0.1\n",
      "Last value: 1.0 Current value: 1.0\n",
      "Minimizer: (0.0, 3.0), Minimum: 1.0 Iterations: 3 with lr: 1.0\n",
      "Last value: nan Current value: nan\n",
      "Minimizer: (nan, nan), Minimum: nan Iterations: 10001 with lr: 10.0\n",
      "Last value: nan Current value: nan\n",
      "Minimizer: (nan, nan), Minimum: nan Iterations: 10001 with lr: 100.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None, None, None, None]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "lr = 0.1\n",
    "\n",
    "def train(lr):\n",
    "  x = tf.Variable(0, dtype=tf.float32)\n",
    "  y = tf.Variable(0, dtype=tf.float32)\n",
    "  thrsh = 0.000001\n",
    "  last_val = None\n",
    "  iteration = 0\n",
    "  while True:\n",
    "    iteration += 1\n",
    "    with tf.GradientTape() as tape:\n",
    "      f = (x-1)**2 + 0.5*(y-3)**2\n",
    "      if(last_val is not None and abs(last_val - f) < thrsh) or iteration > 10000:\n",
    "        print(f\"Last value: {float(last_val)} Current value: {float(f)}\")\n",
    "        break\n",
    "      last_val = f\n",
    "      grads = tape.gradient(f, [x,y])\n",
    "      x.assign_sub(lr*grads[0])\n",
    "      y.assign_sub(lr*grads[1])\n",
    "  print(f\"Minimizer: {(float(x), float(y))}, Minimum: {float(f)} Iterations: {iteration} with lr: {lr}\")\n",
    "\n",
    "\n",
    "\n",
    "lrs = np.logspace(-3, 2, 6)\n",
    "[train(lr) for lr in lrs]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OUR COMMENT\n",
    "We found that the best learning rate for this function is 0.1. It converged the fastest and it found the lowest minimum value of the function.\n",
    "Since we used threshold for stopping the lower learning rates stopped before getting to the same minimizer as the learning rate of 0.1.\n",
    "\n",
    "The higher learning rates were over shooting the local minimum and the function values kept getting higher thus we got the nan values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uRn9T9HjWMzT"
   },
   "source": [
    "###Task 3.3 Estimate regression coefficients by Gradient Descent algorithm "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ORzzZjVwsOdv"
   },
   "source": [
    "In this subtask, we apply Gradient descent algorithm to find the least sqaure estimation of regression coefficients. First, let's import some useful tools to our working space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Px_ZutBvsx_z"
   },
   "source": [
    "We prepare the data by the following code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "MV1QLKaEoKd3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7f6de3a7b190>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "n = 30\n",
    "b0 = 2\n",
    "b1 = 2.5\n",
    "x = tf.random.normal([n,1])\n",
    "error = tf.random.normal([n,1], 0, 0.1)\n",
    "y = b0 + b1*x + error\n",
    "plt.scatter(x.numpy(), y.numpy(), marker='o')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ECWWx2vws3zs"
   },
   "source": [
    "The problem is estimating the regression coefficients from the simulated data using Gradient Descent algorithm. The first thing is write down the objective function and it is the mean square loss\n",
    "  $$\n",
    "    \\frac{1}{n}\\sum_{i=1}^n(y_i - f(x_i))^2\n",
    "  $$\n",
    "where $f(x_i)$ is the regression model. The model we will apply Keras 'Sequential' function, see the codes below. We can see that simple regression model is a special case of a regression Artificial neural network.\n",
    "\n",
    "**The first task of Task 3.3:**\n",
    "\n",
    "In the follwing chunk, you need to write proper codes to complete the defintion of 'model'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "_BTIBX9poeCh"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_3 (Dense)             (None, 1)                 2         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2\n",
      "Trainable params: 2\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import Sequential, layers, optimizers, callbacks\n",
    "model = Sequential([\n",
    "  layers.InputLayer(input_shape=1), \n",
    "  layers.Dense(1)\n",
    "  # you need to write the code to complete the model\n",
    "])\n",
    "model.summary()\n",
    "\n",
    "\n",
    "# Represent the regression a*x + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x2KMpAbVxJV8"
   },
   "source": [
    "**The second task of Task 3.3**: \n",
    "\n",
    "Now we are ready to implement the Gradient descent algorithm to estimate the regression coefficients. As you can see in the follwing chunck, the code is almost there, but you need to write the line 5 by yourself. \n",
    "\n",
    "**Tips**: In TensorFlow, for the square and mean operator, you can call function 'tf.square' and 'tf.reduce_mean'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "rQzk0ia4ogSy"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(16.190037, shape=(), dtype=float32)\n",
      "tf.Tensor(11.252723, shape=(), dtype=float32)\n",
      "tf.Tensor(7.9083548, shape=(), dtype=float32)\n",
      "tf.Tensor(5.6198664, shape=(), dtype=float32)\n",
      "tf.Tensor(4.0371385, shape=(), dtype=float32)\n",
      "tf.Tensor(2.930494, shape=(), dtype=float32)\n",
      "tf.Tensor(2.1481931, shape=(), dtype=float32)\n",
      "tf.Tensor(1.5891836, shape=(), dtype=float32)\n",
      "tf.Tensor(1.185571, shape=(), dtype=float32)\n",
      "tf.Tensor(0.89129883, shape=(), dtype=float32)\n",
      "tf.Tensor(0.67480314, shape=(), dtype=float32)\n",
      "tf.Tensor(0.51421916, shape=(), dtype=float32)\n",
      "tf.Tensor(0.39423424, shape=(), dtype=float32)\n",
      "tf.Tensor(0.30400622, shape=(), dtype=float32)\n",
      "tf.Tensor(0.23577572, shape=(), dtype=float32)\n",
      "tf.Tensor(0.18393199, shape=(), dtype=float32)\n",
      "tf.Tensor(0.14437851, shape=(), dtype=float32)\n",
      "tf.Tensor(0.11409794, shape=(), dtype=float32)\n",
      "tf.Tensor(0.090849295, shape=(), dtype=float32)\n",
      "tf.Tensor(0.072956756, shape=(), dtype=float32)\n",
      "tf.Tensor(0.059159055, shape=(), dtype=float32)\n",
      "tf.Tensor(0.04850144, shape=(), dtype=float32)\n",
      "tf.Tensor(0.040258277, shape=(), dtype=float32)\n",
      "tf.Tensor(0.033875402, shape=(), dtype=float32)\n",
      "tf.Tensor(0.02892857, shape=(), dtype=float32)\n",
      "tf.Tensor(0.02509186, shape=(), dtype=float32)\n",
      "tf.Tensor(0.022114286, shape=(), dtype=float32)\n",
      "tf.Tensor(0.019802367, shape=(), dtype=float32)\n",
      "tf.Tensor(0.018006546, shape=(), dtype=float32)\n",
      "tf.Tensor(0.016611176, shape=(), dtype=float32)\n",
      "b0: [2.0202632]\n",
      "b1: [[2.4056778]]\n"
     ]
    }
   ],
   "source": [
    "lr = 0.1\n",
    "opt = optimizers.SGD(learning_rate=lr)\n",
    "for i in range(30):\n",
    "  with tf.GradientTape() as tape:\n",
    "    loss = tf.reduce_mean((model(x)-y)**2)\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    opt.apply_gradients(zip(grads, model.trainable_variables))\n",
    "  print(loss)\n",
    "print('b0:', model.layers[0].bias.numpy())\n",
    "print('b1:', model.layers[0].kernel.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8ZJkSjc2qWeN"
   },
   "source": [
    "**The third task of Task 3.3**:\n",
    "\n",
    "As you can see that we use build-in optimization solver in TensorFlow to implement the Gradient Descent algorithm in line 8. Nest, you need to write your own code to replace the 8th line.\n",
    "\n",
    "**Tips**: \n",
    "1. In TensorFlow, the model parameters are stored in different layers. In this case, the slope coefficient is stored in 'model.layers[0].kernel' and the intercept term can be found in 'model.layers[0].bias'. As you can see, in TensorFlow, the weights for each neuron is called 'kernel'. \n",
    "2. Both 'model.layers[0].kernel' and 'model.layers[0].bias' are TensorFlow variables and they have method '.assign_sub' as the example in Task 4.2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0.015526666, shape=(), dtype=float32)\n",
      "tf.Tensor(0.014683569, shape=(), dtype=float32)\n",
      "tf.Tensor(0.014028052, shape=(), dtype=float32)\n",
      "tf.Tensor(0.0135182785, shape=(), dtype=float32)\n",
      "tf.Tensor(0.013121829, shape=(), dtype=float32)\n",
      "tf.Tensor(0.012813461, shape=(), dtype=float32)\n",
      "tf.Tensor(0.012573594, shape=(), dtype=float32)\n",
      "tf.Tensor(0.012387009, shape=(), dtype=float32)\n",
      "tf.Tensor(0.012241842, shape=(), dtype=float32)\n",
      "tf.Tensor(0.012128913, shape=(), dtype=float32)\n",
      "tf.Tensor(0.012041054, shape=(), dtype=float32)\n",
      "tf.Tensor(0.011972697, shape=(), dtype=float32)\n",
      "tf.Tensor(0.011919512, shape=(), dtype=float32)\n",
      "tf.Tensor(0.01187813, shape=(), dtype=float32)\n",
      "tf.Tensor(0.011845934, shape=(), dtype=float32)\n",
      "tf.Tensor(0.011820878, shape=(), dtype=float32)\n",
      "tf.Tensor(0.011801388, shape=(), dtype=float32)\n",
      "tf.Tensor(0.011786224, shape=(), dtype=float32)\n",
      "tf.Tensor(0.011774419, shape=(), dtype=float32)\n",
      "tf.Tensor(0.011765243, shape=(), dtype=float32)\n",
      "tf.Tensor(0.01175809, shape=(), dtype=float32)\n",
      "tf.Tensor(0.011752541, shape=(), dtype=float32)\n",
      "tf.Tensor(0.011748214, shape=(), dtype=float32)\n",
      "tf.Tensor(0.011744843, shape=(), dtype=float32)\n",
      "tf.Tensor(0.011742225, shape=(), dtype=float32)\n",
      "tf.Tensor(0.011740183, shape=(), dtype=float32)\n",
      "tf.Tensor(0.011738599, shape=(), dtype=float32)\n",
      "tf.Tensor(0.011737366, shape=(), dtype=float32)\n",
      "tf.Tensor(0.011736409, shape=(), dtype=float32)\n",
      "tf.Tensor(0.011735661, shape=(), dtype=float32)\n",
      "b0: [2.003158]\n",
      "b1: [[2.482111]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "lr = 0.1\n",
    "opt = optimizers.SGD(learning_rate=lr)\n",
    "for i in range(30):\n",
    "  with tf.GradientTape() as tape:\n",
    "    loss = tf.reduce_mean((model(x)-y)**2)\n",
    "    # you need to write the code. \n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    model.layers[0].bias.assign_sub(lr*grads[1])\n",
    "    model.layers[0].kernel.assign_sub(lr*grads[0])\n",
    "  print(loss)\n",
    "print('b0:', model.layers[0].bias.numpy())\n",
    "print('b1:', model.layers[0].kernel.numpy())"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyN/aV/PN6wAM8IZ296g9Amr",
   "collapsed_sections": [
    "jvArLR4rPBE7"
   ],
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3.10.8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
